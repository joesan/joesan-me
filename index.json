[{"body":"","link":"https://bigelectrons.com/","section":"","tags":null,"title":""},{"body":"","link":"https://bigelectrons.com/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://bigelectrons.com/categories/misc/","section":"categories","tags":null,"title":"misc"},{"body":"Recently I had to make a choice of which new car to get so that I can replace my old Diesel car. Given the industry shift to electric vehicles, it is definitely an unfortunate time to buy. In this blog, I wanted to present my simple rational that helped me make my decision.\nNOTE: I cannot vouch for the correctness, and I'm not liable if you use this article for basing your purchase decision!\nFirst, let us see what parameters we are going to use\nParameters to use Purchase Price - The most important parameter that heavily influences which car to buy\nTotal Years of Ownership - For how many years do you want to own your car after purchase\nFuel Price - The cost per liter of fuel at the pipe (be it cost per kWh for BEV or cost per liter for HEV)\nFuel Price Appreciation Percentage - By how much factor does the Fuel price appreciates YoY (Year over Year)\nLiters per 100 km - How many liters for HEV or how much kWh for BEV is required to cover 100 Kilometers\nTotal km over a year - Total Kilometers that will be driven over a year\nMaintenance Costs per year - Average maintenance costs per year\nAdditional Costs - Any other additional costs. For example., installation of a Wallbox at home\nLet us calculate To keep it simple and not bloat this blog article, I have created the calculations in a Scala REPL. Please have a look here\nWhat do the results say I compared a Toyota Corolla Cross 2.0 against aa Tesla Model Y and here are the comparison results:\n1Corolla Cross 2.0 Hybrid: ********** 2Total cost over 10 years with a yearly fuel price appreciation of 10% is: EUR 67978 3Tesla Model Y Long Range: ********** 4Total cost over 10 years with a yearly fuel price appreciation of 2% is: EUR 67502 Assuming that you have tried running the comparison, you can see that by tweaking the parameters for the car of your choice, you will be able to infer a basic estimate on how much you will be spending YoY.\nFor an electric car, I see Tesla Model Y long range as the standard. Anything else (as of today) is not worthy for me to consider. But this may be different to you. So play with different parameters to see the results.\nNext Steps As you might have noticed that I used a pretty much linear appreciation for the price, a much more realistic value would be to estimate the fuel price based on some statistical methods. I'm on it now!\n","link":"https://bigelectrons.com/post/misc/hev-bev-cost-comparison/","section":"post","tags":null,"title":"Operational Cost comparison between a HEV and BEV"},{"body":"","link":"https://bigelectrons.com/post/","section":"post","tags":null,"title":"Posts"},{"body":"Handling missing data is a crucial step in the data preprocessing pipeline for any machine learning project. Imputation, the process of replacing missing data with substituted values, is essential for building robust and reliable models. This article explores various imputation techniques, provides code examples, and discusses the pros and cons of each method.\nUnderstanding Missing Data Before diving into imputation techniques, it's important to understand the types of missing data:\nMissing Completely at Random (MCAR): The missing values have no relationship with any other data or processes. They occur entirely by chance.\nMissing at Random (MAR): The missing values are related to some observed data but not the missing data itself.\nMissing Not at Random (MNAR): The missing values have a pattern or reason behind them.\nChoosing the right imputation method depends on the nature of the missing data and the specific requirements of your project.\nCommon Imputation Techniques Mean Imputation - Mean imputation replaces missing values with the mean of the non-missing values of the feature. 1import pandas as pd 2from sklearn.impute import SimpleImputer 3 4# Example DataFrame 5data = {\u0026#39;feature1\u0026#39;: [1, 2, 3, None, 5], \u0026#39;feature2\u0026#39;: [6, None, 8, 9, 10]} 6df = pd.DataFrame(data) 7 8# Initialize the imputer with mean strategy 9mean_imputer = SimpleImputer(strategy=\u0026#39;mean\u0026#39;) 10 11# Fit and transform the data 12df_mean_imputed = pd.DataFrame(mean_imputer.fit_transform(df), columns=df.columns) 13print(df_mean_imputed) Pros\nSimple and fast to implement. Works well with small datasets. Cons\nReduces data variability. Can lead to biased estimates if data is not MCAR. Median Imputation - Median imputation replaces missing values with the median of the non-missing values of the feature. 1# Initialize the imputer with median strategy 2median_imputer = SimpleImputer(strategy=\u0026#39;median\u0026#39;) 3 4# Fit and transform the data 5df_median_imputed = pd.DataFrame(median_imputer.fit_transform(df), columns=df.columns) 6print(df_median_imputed) Pros\nMore robust to outliers compared to mean imputation. Simple to implement. Cons\nReduces data variability. Can still lead to biased estimates if data is not MCAR. Most Frequent Imputation - Most frequent imputation replaces missing values with the most frequent (mode) value of the feature. 1# Initialize the imputer with most frequent strategy 2mode_imputer = SimpleImputer(strategy=\u0026#39;most_frequent\u0026#39;) 3 4# Fit and transform the data 5df_mode_imputed = pd.DataFrame(mode_imputer.fit_transform(df), columns=df.columns) 6print(df_mode_imputed) Pros\nWorks well with categorical data. Simple to implement. Cons\nCan introduce bias if the mode is not representative. Less effective for continuous data. Constant Imputation - Constant imputation replaces missing values with a constant value specified by the user. 1# Initialize the imputer with constant strategy 2constant_imputer = SimpleImputer(strategy=\u0026#39;constant\u0026#39;, fill_value=0) 3 4# Fit and transform the data 5df_constant_imputed = pd.DataFrame(constant_imputer.fit_transform(df), columns=df.columns) 6print(df_constant_imputed) Pros\nUseful for indicating missingness with a specific value. Simple to implement. Cons\nThe choice of constant value can be arbitrary. May introduce bias if the constant value is not representative. K-Nearest Neighbors (KNN) Imputation - KNN imputation replaces missing values by the mean (or median) of the k-nearest neighbors' values. 1from sklearn.impute import KNNImputer 2 3# Initialize the KNN imputer 4knn_imputer = KNNImputer(n_neighbors=3) 5 6# Fit and transform the data 7df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns) 8print(df_knn_imputed) Pros\nCan capture local patterns in the data. Suitable for both continuous and categorical data. Cons\nComputationally expensive for large datasets. Sensitive to the choice of k and distance metric. Multivariate Imputation by Chained Equations (MICE) - MICE imputes missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. 1from sklearn.experimental import enable_iterative_imputer 2from sklearn.impute import IterativeImputer 3 4# Initialize the MICE imputer 5mice_imputer = IterativeImputer() 6 7# Fit and transform the data 8df_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(df), columns=df.columns) 9print(df_mice_imputed) Pros\nCan handle complex relationships between features. Iteratively refines imputations, improving accuracy. Cons\nComputationally intensive. Requires careful parameter tuning. Backfill and Forward fill Technique In addition to the standard imputation methods like mean, median, and mode, time series data or ordered datasets often benefit from techniques such as Backfill and Forward Fill. These methods leverage the temporal or sequential nature of the data to fill in missing values.\nForward Fill (ffill) Forward fill replaces missing values with the last observed value. This method is useful in time series data where the assumption is that the last observed value is a reasonable estimate for missing values in the near future.\n1import pandas as pd 2import numpy as np 3 4# Example DataFrame 5data = {\u0026#39;date\u0026#39;: pd.date_range(start=\u0026#39;1/1/2020\u0026#39;, periods=10), 6\u0026#39;value\u0026#39;: [1, np.nan, np.nan, 4, 5, np.nan, 7, 8, np.nan, 10]} 7df = pd.DataFrame(data) 8 9# Apply forward fill 10df[\u0026#39;value_ffill\u0026#39;] = df[\u0026#39;value\u0026#39;].ffill() 11print(df) Pros\nSimple to implement. Useful when recent past values are good predictors of current values. Cons\nNot suitable for all types of data. Can propagate errors if the last observed value is incorrect. Backfill (bfill) Backfill replaces missing values with the next observed value. This method assumes that the next available data point is a reasonable estimate for the missing values that came before it.\n1# Apply backfill 2df[\u0026#39;value_bfill\u0026#39;] = df[\u0026#39;value\u0026#39;].bfill() 3print(df) Pros\nSimple to implement. Useful when future values are better indicators of the missing values. Cons\nNot suitable for all types of data. Can propagate errors if the next observed value is incorrect. Combining Forward Fill and Backfill Sometimes, it might be beneficial to combine both forward fill and backfill to ensure that no missing values remain. This can be done sequentially, first applying forward fill and then backfill (or vice versa).\n1# Apply forward fill first, then backfill 2df[\u0026#39;value_ffill_bfill\u0026#39;] = df[\u0026#39;value\u0026#39;].ffill().bfill() 3print(df) Pros\nTemporal Relevance: Suitable for time series and sequential data where order matters. Simplicity: Easy to implement and understand. Data Utilization: Makes full use of available data points to estimate missing values. Cons\nAssumption Dependent: Assumes that the last (forward fill) or next (backfill) observed value is a good estimate, which may not always be true. Error Propagation: Incorrect values can propagate through the dataset, leading to biased estimates. Not Universal: May not be suitable for non-sequential data or data without a natural order. Summary Choosing the right imputation technique depends on the nature of your data and the specific requirements of your machine learning project. Simpler methods like mean, median, and most frequent imputation are easy to implement and work well for many cases. More advanced techniques like KNN and MICE can capture complex patterns but require more computational resources and parameter tuning.\nBy understanding the pros and cons of each imputation method, you can make an informed decision that enhances your model's performance and reliability.\n","link":"https://bigelectrons.com/post/mlandai/handling-missing-data/","section":"post","tags":["ml"],"title":"Handling Missing Data in Data Pre-Processing"},{"body":"","link":"https://bigelectrons.com/tags/ml/","section":"tags","tags":null,"title":"ml"},{"body":"","link":"https://bigelectrons.com/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://bigelectrons.com/categories/technical-stuff/","section":"categories","tags":null,"title":"Technical Stuff"},{"body":"Data leakage is a critical issue in machine learning that can lead to overly optimistic performance metrics and poor generalization to new data. This article explains what data leakage is, why it is problematic, and how to avoid it during data pre-processing.\nWhat is Data Leakage? Data leakage occurs when information from outside the training dataset is used to create the model. This means that the model inadvertently has access to information that it wouldn't normally have in a real-world scenario. As a result, the model's evaluation metrics are misleading because it performs better on the test data than it would on unseen data.\nData Leakage typically happens by applying data preparation techniques to the entire dataset.\nTo keep it simple, when we do data pre-processing we calculate some summary statistics like mean, variance etc., from the data set. Now if we calculate these summary statistics for the entire dataset and use that summary statistics to train the model, and then split the train and test data, the model already knows about the summary statistics of the test data and will perform better. But this is seriously a bad idea as in real world, for some other real world data, your model will suffer terribly!\nWhy is Data Leakage a Problem? Misleading Performance Metrics When a model is trained with leaked information from the test set, it appears to perform exceptionally well during evaluation. This leads to performance metrics that do not accurately reflect the model's true predictive power. Consequently, the model may not perform as well in real-world applications as the evaluation metrics suggest.\nPoor Generalization The primary goal of a machine learning model is to generalize well to new, unseen data. If the model has learned from the test data, it is likely to over-fit to the specifics of that data and fail to generalize. This results in poor performance when the model encounters new data that it hasn't seen before.\nExample of Data Leakage Consider a scenario where you are working with a dataset and want to split it into training and test sets. You then standardize your features using a StandardScaler. The scaler computes the mean and standard deviation of the features to transform them. Here's an example of how data leakage can occur:\nIncorrect Approach: Fitting on Combined Data 1from sklearn.preprocessing import StandardScaler 2from sklearn.model_selection import train_test_split 3import numpy as np 4 5# Example data 6data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) 7target = np.array([0, 1, 0, 1, 0]) 8 9# Split data into training and test sets 10X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42) 11 12# Initialize the scaler 13scaler = StandardScaler() 14 15# Incorrectly fitting on combined data 16combined_data = np.concatenate([X_train, X_test], axis=0) 17scaler.fit(combined_data) 18X_train_scaled = scaler.transform(X_train) 19X_test_scaled = scaler.transform(X_test) In this example, the scaler uses information from the test set to calculate the mean and standard deviation. When the test set is transformed, it's no longer an independent set, leading to misleadingly high performance metrics.\nCorrect Approach: Separate Fitting and Transformation 1# Initialize the scaler 2scaler = StandardScaler() 3 4# Fit and transform the training data 5X_train_scaled = scaler.fit_transform(X_train) 6 7# Only transform the test data 8X_test_scaled = scaler.transform(X_test) By fitting the scaler only on the training data, we ensure that the test data remains unseen during training, providing a more accurate evaluation of the model's performance.\nPractical Implications of Data Leakage Model Evaluation When you avoid data leakage, the performance metrics (such as accuracy, precision, recall, etc.) reflect the model's true ability to generalize to new data. This gives a realistic measure of the model's effectiveness.\nAvoiding Over-fitting Preventing data leakage helps the model avoid over-fitting to the specifics of the test data. This ensures that the model learns patterns that generalize well to new, unseen data.\nReal-World Application Models that are evaluated without data leakage are more likely to perform well in real-world scenarios. They are robust and reliable because they have been tested on truly unseen data.\nSummary Data leakage can significantly impact the performance and generalization of a machine learning model. By carefully separating the training and test data during data pre-processing, and ensuring that transformations are fitted only on the training data, we can avoid data leakage. This practice leads to more accurate performance metrics and better generalization to new data.\nBy adhering to these principles, you can build robust machine learning models that perform well in real-world applications and provide reliable performance estimates.\n","link":"https://bigelectrons.com/post/mlandai/data-leakage-in-ml/","section":"post","tags":["ml"],"title":"Understanding Data Leakage in the ML Paradigm"},{"body":"open-electrons is an attempt by myself to abstract all e-mobility related protocols, use cases and much more. In the E-Mobility industry, there are some well known protocols which are industry standard. With the open-electrons project, my aim is to provide a software stack which can be customized and deployed to mimic the functionality of an eMSP (eMobility Service Provider).\nFor more information, have a look here for some basic documentation and here for the source code. Please note that not all the project source is in public mode. Fell free to get in touch with me for access to the projects.\n","link":"https://bigelectrons.com/post/projects/project-open-electrons/","section":"post","tags":["open-electrons"],"title":"About open-electrons project"},{"body":"","link":"https://bigelectrons.com/tags/open-electrons/","section":"tags","tags":null,"title":"open-electrons"},{"body":"","link":"https://bigelectrons.com/post/projects/","section":"post","tags":null,"title":"Projects"},{"body":"I just came back from my vacation in Paris. Just thought of writing down my experience and what I did with my family while in Paris.\nHow to Reach Paris We decided that we will drive to Paris. From where we are located, it just takes us a 6 to 7-Hour drive by car. On the way, there are plenty of rest places with sufficient facilities for food.\nWhere to Stay There are tons and tons of options. I just found a hotel randomly at booking.com near Malakoff, and it is about 10 to 15 Minutes away (with the Metro) from the attractions in Paris (Eiffel Tower, Louvre Museum). If you are planning a trip to Paris, better find a hotel that offers you a private car park. It is better to avoid using your car to explore Paris due to safety concerns, and the other main reason being that the Metro in Paris is one of the best and efficient ways to move around.\nWhat to do There are tons of places that you could visit when in Paris. You only need the time to do all that. For us, it was just 3 full days, so we nailed it down to a few spots like the Eiffel Tower, the Louvre Museum, the Galeries Lafayette, the Parc des Buttes-Chaumont, the Moulin Rogue, the Cathédrale Notre-Dame. That is optimistically ambitious to visit all these places in a span of 3 days.\nWith 2 full days and one half a day to explore Paris, we thought of doing two attractions every day, but man we were wrong. It was so crowded everywhere due to the holiday season, and we had to wait hours together to get into the attractions. Soon after my first experience on day 1 at the Eiffel Tower, I realized that all the 6 places that I shortlisted is indeed not doable in 3 days. So we decided to do the Eiffel Tower, Galaries Lafayette, the Louvre and of course catch a glimpse of Tour de France.\nThe Eifel Tower Our first stop was the Eiffel Tower, one of the world wonders. I was unable to secure any online tickets before my visit, so we had to wait at the queue to secure an entry, and we had to wait for about roughly 2 Hours to get to the ticket counter. I thought everything after that would be quick, but I was wrong again. We had to wait for another 30 to 45 minutes to get into one of the elevators that would take us to the second floor. Once in the second floor, we spent some time gazing at the city of Paris. Again here getting access to the view point was a challenge because of the huge crowd. However, after a few minutes of struggle and changing positions, we were able to secure a spot to view the cits of Paris.\nAfter clicking some snaps, we moved to the lift to take us to the summit. Again some waiting time! Once at the summit, it was more or less the same situation as on the second floor. If you are fear of heights, the summit is not for you. We clicked some snaps and tried to locate the other attractions in Paris. We were able to spot a few of them. Overall, we enjoyed visiting the Eiffel tower minus the huge crowds. We came down to the second floor, and we decided to take the stairs. It was fun doing so!\nWe were so tired after the drive, the wait and the visit to the Eiffel tower. We went back to our rooms to retire for the day. It was so warm, sultry and hot the entire day.\nGaleries Lafayette The original plan for the day (was a Sunday) was to visit the Louvre museum, but somehow I decided to visit the Lafayette. I had to fight a migraine that had set in because of yesterday. We started from the hotel around 9:30 and reached Lafayette in about 30 minutes. We fell in love with the Metro in Paris. It was easy to get by yourself with little to no help. After reaching Lafayette, we realised that it opens only at 11:00 on Sundays. But never mind, we went into the McDonald's nearby and had our breakfast.\nThe shops nearby started to open, and we headed directly to the Lafayette. It is quite a big shopping complex, kind of like a \u0026quot;Open Office\u0026quot; space for shopping where you can see lots and lots of brands having their presence in a shared space. At the ground floor you can find those big names (quite expensive) retailers. I even saw a Leica shop. Knowing that I will not be buying anything from them, we headed to the upper floor. There were quite a few reasonable shops selling at reasonable prices. After spending a few hours, we decided to move on, given that the \u0026quot;Tour de France 2022\u0026quot; is happening.\nTour de France We took the metro from a station nearby Galaries Lafayette and headed towards Champs Élysées where I understood is the Sunday's venue for Tour de France. The metro station at Champs Élysées was blocked, and we have to get down at a station before, and we had to walk towards Champs Élysées. The day was so humid and sunny, but we defied all that and headed towards a safe spot from where we could watch the race. It was so crowded that finding a spot next to the race fence along the street was almost impossible. But we nevertheless took refuge over a bridge from where we had a clear view of the cyclists. It was the closest that I could get. We spent around 2 to 3 Hours and then started to walk towards the Metro to retire for the day.\nThe Louvre Museum This place is a mandatory visit for every tourist to Paris, for the obvious reason that the world-famous Mona Lisa art is hosted there. So our third day in Paris is dedicated to visiting the Louvre Museum. As usual, we did not have any online tickets purchased and that meant that we had to wait in the open queue for hours together. It took us almost 2 hours wait in the queue to enter the Museum after which we headed to the ticket section which was to my surprise not so crowded. After purchasing the tickets, we refreshed ourselves at the restaurant located inside the Museum. The Museum is divided in 3 sections called wings. Without any hesitation, we first went to the Denon Wing where the Mona Lisa is housed. Seems everyone at the Museum were also interested to see the Mona Lisa, and we ended up again standing in the queue. As we had our chance to stand in front of Mona Lisa, I just starred at her for a few minutes, clicked some photos, and we were told by the security personnel to move on. What a stunning painting it was!\nIt was not only Mona Lisa, but the other massive paintings that were on display really caught out attention. I could not imagine how such art were produced few hundred years ago and even more surprising, how they have been preserved till date. We then went to the other wings and had a nice time exploring all the artefacts and to get to know about its history. No wonder why the Louvre was so crowded. It felt like an endless place for historic art paintings, sculptures and antiquities. Other than the crowd, our day at the Louvre was indeed spent well.\nBoat Ride Since this will be ouf final day at Paris before we head over to Strasbourg, we decided to finish the day with a boat ride, and we already have selected a place (Pont Neuf) and an operator (Vedettes). Again we did not have any tickets, but we just headed straight to the ticket counter and booked a ride that would start in an Hour. It is a round trip with live explanation of the history of some buildings along the way and the main tourist attractions. It was a nice opportunity for us to get to know about some history and to click some beautiful pictures. The ride lasted for about 50 minutes, and it was well worth it.\nWe headed back to our hotel with such fond memories of our days in Paris. The memories will be etched forever in our thoughts. May be, we come back once more at some point in the future. Do visit Paris if you are still deliberating to do so! It is worth it.\n","link":"https://bigelectrons.com/post/misc/vacation-in-paris/","section":"post","tags":null,"title":"Vacation in Paris"},{"body":"I occasionally get physically active and do some training. I was figuring out what kind of physical activity that I could do when I'm sitting. This is when I thought about doing some exercises for the forearm.\nAs I'm someone who loves to DIY, I wanted to figure out which actions I want my forearm to do, and I remember being a teenager doing some dumbbell training that had to be rolled with both your arms stretched out. I wanted something kind of similar to that, but not having to stand up which is when I came up with this thought of putting some simple parts together and construct a stick grenade like device that can be used.\nHere is what I came up with:\nAll the parts used are sourced from a local hardware store, but nevertheless here is a quick summary of what I used:\nGalvanized steel nipple - 180mm in length and has a diameter of around 3.5cm Steel caps for both the ends as per the dimensions Range extender that goes from 3.5cm to 5cm in diamater Small steel nipple that can be attached to the range extender - 10cm in length and 5cm in diameter With a few rubber dampeners, I have the whole set up ready. Now all what I need is a Olympic sized weight plate that I need to insert on the top end of this stick. I hope to pue this one to good use when I'm sitting resting!\n","link":"https://bigelectrons.com/post/misc/diy-forearm-trainer/","section":"post","tags":null,"title":"A DIY Forearm Trainer Kit"},{"body":"","link":"https://bigelectrons.com/categories/miscellaneous/","section":"categories","tags":null,"title":"Miscellaneous"},{"body":"Let us debunk this major, but simple functional abstraction called the Functor\nIntuition behind Functors The idea stems from category theory and can be readily applied to elegantly look inside a container that contains a certain category and apply a function to the contents, thereby producing another category wrapped inside the same container. The name of a functor sounds very much similar to that of a function and in fact it is indeed the same, but there is this additional thingy called the container or in other words a context.\nYou might already be familiar with such contexts like List[A], Seq[A] that can hold zero to many values of type A. List or Seq can be said that they are the containers for holding elements or types of type A. Other containers or contexts like Option, Either, Future etc., also can be cited. Henceforth, we will use the term context instead of a container, but both mean the same.\nHitting the ground To manipulate the data inside a context could be done like this:\n1 // Let us use the Option as our context 2 val somePerson = Some(Person(\u0026#34;firstName\u0026#34;, \u0026#34;lastName\u0026#34;, salary = 1000000)) 3 if (somePerson.isDefined) { 4 somePerson.get.map(person =\u0026gt; person.copy(salary = 20000000)) 5 } So, with that example., we simply checked if a Person exists and if yes, we manipulated his salary. That piece of code is like telling to scala how to do, which is basically an imperative style of programming. We instead would want to say to Scala just what to do and, the programming language should figure out how to do. This is where our Functor intuition could help us out.\nMost of scala's context have built in functor like capabilities. Let us get to the following example:\n1 val listOfSalary = List(200000, 100000, 400000, 600000, 700000, 350000) 2 val bonusFunction: Int =\u0026gt; Int = baseSalary =\u0026gt; baseSalary + 100000 3 val finalSalaries = listOfSalary.map(bonusFunction) With that code snippet, we have instructed Scala what we want which is to map the salaries to bonuses. The map function figures out how to get the job done. Its signature is like this:\n1 trait F[A] { 2 def map[A, B](f: A =\u0026gt; B): F[B] 3 } Here F[A] is our container or context in which we have defined a map function that should know how to map from A -\u0026gt; B\nLet us now define a simple Functor. You need three things to do that:\nThe container / context that you will plug in (List, Try, Either, Option etc.,) - F[_] The Type from which you want to go from - A The Type to which you want to go to - B With that said, let us implement a Functor interface:\n1 import scala.language.higherKinds 2 trait Functor[F[_]] { 3 def map[A, B](fa: F[A])(f: A =\u0026gt; B): F[B] 4 } What we have basically done is, we have created a trait that operates on a generic context which is the F[_]. If you follow what higher kinded types in Scala mean, that signature should be straight forward. It basically says that in the place of F[_], I can plugin any context which itself is a context around a certain type. For example., List, Option, Try, Either all are contexts that around a certain type. So they qualify to be the F[_] which can be exemplified as F[List], F[Option], F[Try], F[Either] and so on. Have a lok here for some basic understanding of higher kinded types\nFor the sake of this blog, let us implement a List Functor as below:\n1 val listFunctor: Functor[List] = new Functor[List] { 2 def map[A, B](fa: List[A])(f: A =\u0026gt; B): List[B] = fa.map(f) 3 } With that, we have now defined a context around List that knows how to map one category in a List to another category. I hope this was clear enough.\nFunctor laws To qualify for Functorship, the following laws should be respected:\nIdentity This basically states that when a map is called on a Functor with an identity function, you get the same Functor back. Remember that an identity function is one that returns the exact same input. So basically, we are saying that:\n1 Functor[X].map(identity) \u0026#34;should be equal to\u0026#34; Functor[X] Associativity If f and g are two independent functions, then calling a map on f anf then g is as good as calling a map with g composed f or in other words calling g(f(x)). So basically, we are saying that:\n1Functor[X].map(f).map(g) == Functor[X].map(x =\u0026gt; g(f(x)) Wrap up So essentially a Functor is just doing this:\nUnwrap a value from a context Apply a function to that context Rewrap that value back into that context Return the new context That is all what is to Functors. Really simple indeed!\n","link":"https://bigelectrons.com/post/scala/scala-functors/","section":"post","tags":null,"title":"Functors in Scala"},{"body":"Aren't you at your limits dealing with XML? If so why still bother using a build tool that still uses an outdated format like the XML? Why use Maven when you can be friends with sbt? Here is a small script that helps you to convert your pom.xml to a build.sbt. The generated build.sbt is rusty, and you need to adapt it after it is converted.\n1#!/bin/sh 2exec /opt/softwares/scala-2.12.2/bin/scala \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; 3!# 4 5/*** 6scalaVersion := \u0026#34;2.12.2\u0026#34; 7 8libraryDependencies ++= Seq( 9 \u0026#34;org.scala-lang.modules\u0026#34; %% \u0026#34;scala-xml\u0026#34; % \u0026#34;2.0.0\u0026#34; 10) 11*/ 12 13import scala.xml._ 14 15val lines = (XML.load(\u0026#34;pom.xml\u0026#34;) \\\\ \u0026#34;dependencies\u0026#34;) \\ \u0026#34;dependency\u0026#34; map { dependency =\u0026gt; 16 val groupId = (dependency \\ \u0026#34;groupId\u0026#34;).text 17 val artifactId = (dependency \\ \u0026#34;artifactId\u0026#34;).text 18 val version = (dependency \\ \u0026#34;version\u0026#34;).text 19 val scope = (dependency \\ \u0026#34;scope\u0026#34;).text 20 val classifier = (dependency \\ \u0026#34;classifier\u0026#34;).text 21 val artifactValName: String = artifactId.replaceAll(\u0026#34;[-\\\\.]\u0026#34;, \u0026#34;_\u0026#34;) 22 23 val scope2 = scope match { 24 case \u0026#34;\u0026#34; =\u0026gt; \u0026#34;\u0026#34; 25 case _ =\u0026gt; s\u0026#34;\u0026#34;\u0026#34; % \u0026#34;$scope\u0026#34;\u0026#34;\u0026#34;\u0026#34; 26 } 27 28 s\u0026#34;\u0026#34;\u0026#34; \u0026#34;$groupId\u0026#34; % \u0026#34;$artifactId\u0026#34; % \u0026#34;$version\u0026#34;$scope2\u0026#34;\u0026#34;\u0026#34; 29} 30 31val buildSbt = io.Source.fromURL(\u0026#34;https://gist.githubusercontent.com/joesan/59d36606f0529af148000d54202eb370/raw/f854aa443686051637068667401d1e9a412ad192/build.sbt\u0026#34;).mkString 32val libText = \u0026#34;libraryDependencies ++= Seq(\u0026#34; 33val buildSbt2 = buildSbt.replace(libText, libText + lines.mkString(\u0026#34;\\n\u0026#34;, \u0026#34;,\\n\u0026#34;, \u0026#34;\u0026#34;)) Save this file as convert.sh in the same location where your pom.xml is located\nCalling it is pretty simple!\n1./convert.sh \u0026gt; build.sbt One thing that is worth mentioning that XML support for Scala was dropped with Scala versions 2.13 and up, so to be able to run this script, you need to have Scala 2.12 installed. I have Scala 2.12.2 in my path, and you can see that in this line here:\n1exec /opt/softwares/scala-2.12.2/bin/scala \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; For now ignore the two parameters, \u0026quot;$0\u0026quot; \u0026quot;$@\u0026quot; as they are there if you need to pass parameters from outside. So there you have it. Instant conversion of pom.xml to build.sbt. If you need to pass in additional parameters, use these placeholders to inject them into the Scala code.\n","link":"https://bigelectrons.com/post/scala/scala-pom-to-sbt/","section":"post","tags":null,"title":"Convert your pom xml to build sbt"},{"body":"","link":"https://bigelectrons.com/tags/guitar/","section":"tags","tags":null,"title":"guitar"},{"body":"Climbing the Pentatonic scales Recently, I can say that I made very good progress playing across the neck of my guitar. I memorized few pentatonic shapes, but I was stuck the whole while on those specific boxes. The whole thing did not make any sense to me. Just wandering within those boxes is not pleasant music. Frustrated, I looked at Youtube for some motivational ideas on how to unstuck me from the boxes.\nLuckily, this one video in Youtube that I found on a random search helped me immensely.\nThe idea is to think in terms of triangles and move back and forth to those triangles. Man this was wonderful, and I now kind of get the idea! This helped me a lot in playing across the neck of my guitar. I now need to practice this for speed and even more accuracy. If you feel like you are stuck in a box, that video there is a real eyeopener. Go watch it!\n","link":"https://bigelectrons.com/post/misc/guitar-navigating-pentatonic-scale/","section":"post","tags":["guitar"],"title":"Navigating the Pentatonic Scale in Triangles"},{"body":"","link":"https://bigelectrons.com/tags/boots/","section":"tags","tags":null,"title":"boots"},{"body":"Disclaimer: I'm not paid to write about this post, this is purely my ownership experience using these boots!\nA blog on a pair of shoes? A bit unusual for my blog that I'm writing about a pair of shoes. It is still just a shoe, but it is a Red Wing Chukka, the 3140 in the Oro-Iginal leather. There is a reason why I'm writing about this piece of hardware that brings joy and comfort to my knee and foot. I have experienced pain in my left knee for days together after walking with my current shoe and trust me, this is no fun. I was in serious need of a pair that is rugged and at the same time easy on my knee. Enter the Red Wing Work Chukka 3140!\nI went with this Oro-Iginal leather as I find the color to be more versatile. It should look good in a casual setting and should give me a contrast look when paired with my raw denim pants. I'm not new to Red Wing shoes as I have owned a pair of the that I have been using for a few years until the recent flooding to which I lost it.\nWhy do I want to write a blog article about this? The main reason being that I have been struggling with my pair of shoes that I have purchased from the local shoe store which is making my knee problems worse. If you scroll through a few posts from the past, you will see that I do not have knee ligaments on my left knee as a result of an MTB accident 2 years ago. Most of the shoes that I have been using are making the problems worse. I needed a pair that is leather enough to last for decades and at the same time is soft to walk long distances. Red Wing Chukkas kind of tick both the boxes for me. It is this combination of the leather, and the tread sole that made me to purchase this pair.\nSizing You need to be careful with sizing as some Red Wing models like their Moc Toe, or the Iron Ranger runs relatively large primarily because of the last that is being used, but the Work Chukka's use a completely different last which is a bit narrow and long. Here is the rule to size your Red Wing boots. With the Moc Toe and the Iron Ranger, size a full size down from your normal shoe size. With the Work Chukka, go a half size down from your normal shoe size.\nIn my case, I'm a US 11 on a normal shoe that you get from the ubiquitous shoe store, hence I landed a US size 10.5 on my Chukka. I can wear a think or a thin socks and I have no complaints!\nLeather The leather Oil-tanned, meaning that the leather is impregnated with oil during it's construction. It is equivalent to water proofing the leather to a certain degree. Oil-Tanned leather is exceptionally durable, pliable and soft. When new it starts as a bright Orange and as you wear, it gets scuffed and buffed against the environment and evolves a natural patina that kind of darkens it to a certain extent.\nIf you take proper care of the leather, it should you last you forever.\nComfort \u0026amp; Durability Nope! It is not comfortable the very first moment you have them on your foot. It takes some taming. Be ready to have some pain, discomfort for the first few days. Thankfully, breaking-in the Work Chukkas is much easier than breaking-in a pair of Moc Toe, or the Iron Ranger. If you get your sizing nailed on, in just a few days time and wear you will have your most comfortable pair of quality leather boots ever.\nThere are two available widths to choose from, the D or the EE where the latter is a bit narrow compared to the wider EE width. I absolutely have no complaints on the D width. The fit like a glove.\nThe shoe is 360° Good Year welt, a construction method introduced in the 19th century and still widely used. This construction gives these boots extreme durability.\nThankfully, I had a pair of laces lying around which was longer by a few inches. The stock ones that came with the shoe is a bit smaller. I just replaced it with the Moc Toe laces that I had as a spare.\nThe soles in the Work Chukka is not stitched to the welt like in the Moc Toe, but rather is just glued. The glue has a tendency to come off after a certain period of time. I have seen people complaining of the sloe detaching from the shoe. But this is not an issue as it can be resoled countless number of times (though they incur some cost). There is a possibility to resole this in such a way that it can be stitched to the welt. I will do this once my sole wears out.\nThere is a leather inner sole that gets shaped to your foot after a few months of wear. You can literally feel your toe marks after a few months of wear indicating that the shoes are truly yours. It is a bit uncomfortable when you start as your foot has to wrestle against some hard leather, but trust me it gets smooth (just like every other relationship) after few interactions.\nA mention on the outer sole which is where the magic for my knee happens. It is super soft that is so very comfortable when walking long distances.\nSo there it is! The Red Wing 3140 Chukka in Oro-Iginal leather that I can use for decades to come.\n","link":"https://bigelectrons.com/post/misc/red-wing-3140/","section":"post","tags":["boots"],"title":"Red Wing 3140 Work Chukka"},{"body":"","link":"https://bigelectrons.com/tags/hugo/","section":"tags","tags":null,"title":"hugo"},{"body":"What happened? It was time to move again! In one of my earlier posts, I have mentioned my move from Blogger to GH-Pages and for that I decided to go with Zola. It was all fine, but after a few months, I wanted to have few additional features for the theme that I was using. It was such a great and a simple theme and looked so elegant, but some features that I was in dire need of was kind of missing.\nSo I decided that it was time to move on and started to explore on Hugo. I came across this yet another beautiful theme that I could not resist as it was ticking the boxes for all what I want (as of today). Without hesitation, I started to migrate my blog content, and the CI / CD build to reflect this new theme that is based on Hugo. After some hurdles, I was able to move everything from left (Zola) to right (Hugo) in just a day.\nKudos to these SSG's (Static Site Generators), they are really awesome.\n","link":"https://bigelectrons.com/post/misc/moved-from-zola-to-hugo/","section":"post","tags":["zola","hugo"],"title":"Moved From Zola to Hugo"},{"body":"","link":"https://bigelectrons.com/tags/zola/","section":"tags","tags":null,"title":"zola"},{"body":"I had a necessity to generate a PDF document out of a set of jpg images that I had on my local hard drive on my Ubuntu 20.04 Laptop. I came across this simple step on how to do it. Here it is documented:\nInstall img2pdf 1sudo apt-get install -y img2pdf Navigate to the folder that contains your images and run the following command 1img2pdf *.jp* --output your-pdf.pdf As usual, img2pdf --help is your friend for additional options on how to use this package. Just for a reference, here is the link to the GitHub project.\nIf img2pdf results in a huge file (which it does) size as there is no compression happening. I'm yet to figure out how to get the resulting pdf smaller. Here is an alternative that you could use, but it comes with a graphical user interface!\n1sudo apt-get install gscan2pdf ","link":"https://bigelectrons.com/post/linux/images-to-pdf/","section":"post","tags":["ubuntu","images","pdf"],"title":"Convert Images to PDF on Ubuntu"},{"body":"","link":"https://bigelectrons.com/tags/images/","section":"tags","tags":null,"title":"images"},{"body":"","link":"https://bigelectrons.com/categories/linux/","section":"categories","tags":null,"title":"linux"},{"body":"","link":"https://bigelectrons.com/tags/pdf/","section":"tags","tags":null,"title":"pdf"},{"body":"","link":"https://bigelectrons.com/tags/ubuntu/","section":"tags","tags":null,"title":"ubuntu"},{"body":"Sometimes you have the need to run a shell script for some basic functions or simple tasks that you want to execute locally on your machine. The goto choice will be to write some shell script (assuming you are on a Linux). Here is a way to do that but use Scala, your favorite language.\nIt is actually quite simple as it can be seen from the snippet below:\n1#!/bin/sh 2exec scala \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; 3!# 4object HelloShell { 5 def main(args: Array[String]) { 6 println(\u0026#34;Hello, shell! \u0026#34; + args.toList) 7 } 8} Worth mentioning is this line exec scala \u0026quot;$0\u0026quot; \u0026quot;$@\u0026quot; where:\n0 Expands to the name of the shell script\n@ Expands to the positional parameters, starting from one\nCalling it is pretty simple! Save it as helloShell and run:\n1./helloShell.sh shebang Apart from the Shebang, there is nothing specific to the Shell script! Everything underneath is just pure Scala. The goal of this article is to showcase a small need that I had with respect to my blogging content. I frequently have the need to write about certain topics, but I could not get them completed in one sitting. I often times have to keep these new articles / blogs pending for quite some time, until I get the time to work on them. Fortunately, Zola the underlying engine that I use for powering my blogging content, has a neat mechanism to indicate any unfinished content as a draft.\nAs every content here is written as a Markdown content, it starts with some metadata that the Zola engine leverages to generate html. Here is a sample:\n1+++ 2title=\u0026#34;some title\u0026#34; 3description=\u0026#34;Some description\u0026#34; 4date=2021-04-06 5draft=true 6 7[taxonomies] 8categories = [\u0026#34;Technical Stuff\u0026#34;] 9tags = [\u0026#34;distributed systems\u0026#34;] 10+++ 11 12The actual blog content starts here.... Have a look here for the content of this page as a Markdown. Ok so what has this got to do with this article? Well, nothing much, but I would want to know what all content has been marked with draft=true so that I know which ones I should come back later and finish the writing. My love for Scala forced me to write this search as a script.\nYou can find the script here. To call the script, we just do:\n1./list_draft_content content draftStatus Where, the first argument is the folder where we want to recursively search for markdown files, and the second argument is the name of the file where we want to write all the entries that are found! Quite simple!\nI have also integrated this in my GitHub Action workflow as a job (listDraftContent) which will then run this script and write a file called draftStatus.txt as a new branch called draft-status.\nCool or?\n","link":"https://bigelectrons.com/post/scala/scala-shell-script/","section":"post","tags":null,"title":"Running Scala from within a shell script"},{"body":"Definition Simply put, a Bloom filter is a space-efficient probabilistic data structure with which we can determine the probable existence of a certain thing in a certain data set, and we can determine the non-existence of a certain thing in a certain data set with utmost accuracy. Doing all this in a memory space efficient manner.\nIn a gist, Bloom filters are about determining if an element may be in a set or is definitely not in a set\nAllowed Operations You can only do one of the two operations on a given Bloom filter. You could still extend the Bloom filter to remove elements, but we stick to these two operations for discussion’s sake:\nInsert a new element - add\nCheck for an element - verify\nMechanics The Bloom filter works by populating a bit Vector of a specific length. To understand Bloom filter, we need to understand Hashing. You might have used Hashing in a cryptographic context where you take a data of arbitrary size, run it through some algorithm, and you get back a value that is of a fixed size or often times called the hash value. Most of the time such hash functions are one way operations. Why do we care about Hashing by the way for discussing Bloom filters? Using a simple non-cryptographic hash such as the murmur3 (multiply, rotate, multiply, rotate) is a good choice for doing lookup tasks based on hashes. It is a simple and a fast algorithm and finds its usage in common scenarios like checksum, id generators. So this becomes a natural choice for using with Bloom filters.\nIt is all about doing some math with the following four variables:\nn: Represents the number of input elements\nm: Represents the memory used by the bit-array or in other words, the size of the array\nk: The total number of hash functions to be used\np: The probability of a false positive\nThe image below shows an empty Bloom filter with a bit array of size m (here 10 elements), initialized to 0 for all the elements.\nLet us assume that for simplicity we just have k (the number of hash functions) set to 2, and our hash functions looks like this:\nHash Function 1 (H1) = input mod 2 =\u0026gt; H1(“foo”) % 2 = 2\nHash Function 2 (H2) = input mod 6 =\u0026gt; H2(“foo”) % 6 = 6\nThe result of the hash functions mean that we will set the corresponding bits in the Bloom filter array to 1. For example., at the bit positions 2 and 6, we will flip the values to 1 as seen in the image below:\nLet us now do the same for another entry:\nHash Function 1 (H1) = input mod 2 =\u0026gt; H1(“bar”) % 2 = 4\nHash Function 2 (H2) = input mod 6 =\u0026gt; H2(“bar”) % 6 = 8\nWe then set our Bloom filter indices accordingly, which is shown in the image below:\nThat's it, for our example's sake, we are done with insertion. Let us now check for the existence of a certain element in the Bloom filter! The lookup works more or less the same way as an insert where we run the text to be looked up through the hash functions, check if the indices are set to 1. If all the indices are set to 1, then we can say that particular text is probably present. If in case any one of the indices is not set to 1, then we can for sure say that the text is not present.\nLet's test it for the text foo:\nHash Function 1 (H1) = input mod 2 =\u0026gt; H1(“foo”) % 2 = 2\nHash Function 2 (H2) = input mod 6 =\u0026gt; H2(“foo”) % 6 = 6\nWe can see from the populated Bloom filter that the indices at positions 2 and 6 are set to 1, so we have a potential false positive or a true positive or in other words we can probably say that the text might be present.\nLet's test if for the text nope:\nHash Function 1 (H1) = input mod 2 =\u0026gt; H1(“nope”) % 2 = 2\nHash Function 2 (H2) = input mod 6 =\u0026gt; H2(“nope”) % 6 = 5\nThere we have a 0 at index 5, so we have a potential true negative or in other words we can for sure say that this text is not present. Yes, it is that simple!\nWe can indeed tweak the output of the Bloom filter by adjusting the size of the bit set. By adding more elements to the bit set, or the number of indices, we reduce the eventual probability of getting false positives. Probability of getting a false positive also decreases by increasing the number of hash functions.\nThe probability that a hash function sets an index to 1 in a set of m elements is given by:\n$ P(1) = \\frac{1}{m} -----\u0026gt; eq (1)\nThe probability that a given hash function fails to set an index to 1 in a set of m elements is given by:\n$ P(0) = 1 - P(1) $ -----\u0026gt; eq (2)\nFrom the laws of Probabilities, the sum of probabilities add up to 1, hence:\n$ P(0) + P(1) = 1 $ rearranging, we get $ P(0) = 1 - P(1) $ -----\u0026gt; eq (3)\nSo we end up with the following probability for one hash function failing to set a given index or bit to 1:\n$ P(0) = 1 - \\frac{1}{m} $ -----\u0026gt; eq (4)\nHence, after we have inserted all the elements (say n) in the Bloom filter, by running through k number of hash functions, the probability that a particular index is still 0 is given by the following equation:\n$ P(0) = (1 - \\frac{1}{m})^{kn} $ -----\u0026gt; eq (5)\nThe above equation is based on the assumption that the hash functions are independent of each other. Now by substituting this value for P(0) in {{ textcolor(color=\u0026quot;red\u0026quot; text=\u0026quot;eq (2)\u0026quot;) }} and doing a bit of re-arranging, we get the probability of a false positive:\n$ P(1) = (1- [1 - \\frac{1}{m}]^{kn})^{k} $ -----\u0026gt; eq (6)\nWith that being said, how do we choose the number of hash functions and the size of the bit array for the Bloom filter. It turns out that there are some formulas that hold good to determine them:\n$ m = -\\frac{n*lnP}{(ln2)^{2}} $ -----\u0026gt; eq (7)\n$ k = \\frac{m}{n}(ln2) $ -----\u0026gt; eq (8)\nI'm trying to figure out how eq (7) and eq (8) works out, but for the sake of this article, we can use them to determine one value from the other.\nHash Functions The choice of the hash functions that will be used should be independent of each other, i.,e the outcome of one of the hash function should not have any effect on the outcome of the other hash functions. The opted hash functions should be fast. It is recommended to use non-cryptographic hash functions like the murmur hash. Have a look here on why murmur3 should not be used in a security context.\nThe performance of the Bloom filter is directly proportional to the number of hash functions used, as there is a tendency for the Bloom filter to become slow when more hash functions are used.\nI will probably revisit this content later on, but for now I'll wrap it up here!\nApplication \u0026amp; Usage Where do we use Bloom filters - Everywhere where we want to do a look up and everywhere where the false positives can be tolerated!\nFurther Reading A definitive article on Bloom filters could be found here\nFor an interactive show on how Bloom filters work, have a look here\n","link":"https://bigelectrons.com/post/mlandai/bloom-filter/","section":"post","tags":["ml"],"title":"When a Bloom filter really blooms...."},{"body":"","link":"https://bigelectrons.com/tags/git/","section":"tags","tags":null,"title":"git"},{"body":"If you have forked a repository in GitHub that you want to work on, you do so, make your changes locally and once you are confident that everything works, you issue a pull request to the upstream so that your changes can be reviewed and merged. Here is a small snippet that I use to keep my fork up-to-date with the upstream.\nupstream - The original repository from which you fork\nStep 1: Add the remote upstream repository\n1git remote add upstream https://github.com/ORIGINAL_OWNER_NAME/ORIGINAL_REPO_NAME.git Step 2: Check if upstream is referenced\n1git remote -v should print out the following:\n1origin https://github.com/FORKED_OWNER_NAME/ORIGINAL_REPO_NAME.git (fetch) 2origin https://github.com/FORKED_OWNER_NAME/ORIGINAL_REPO_NAME.git (push) 3upstream https://github.com/ORIGINAL_OWNER_NAME/ORIGINAL_REPO_NAME.git (fetch) 4upstream https://github.com/ORIGINAL_OWNER_NAME/ORIGINAL_REPO_NAME.git (push) Step 3: Run the following commands to update forked repo with changes from original repo\n1git fetch upstream Step 4: Checkout master branch from the local fork\n1git checkout master Step 5: Merge changes from upstream/master into local master form\n1git merge upstream/master Step 6: To check if your local forked master is up-to-date with upstream master, run the following\n1git fetch upstream \u0026amp;\u0026amp; git diff remotes/upstream/master master \u0026gt; changes.diff If there are no changes, then the diff file should be empty. You could combine all that in a script. That's it!\n","link":"https://bigelectrons.com/post/infra/git-upstream-origin-fork/","section":"post","tags":["git"],"title":"Keeping up with Upstream Git Repo"},{"body":"","link":"https://bigelectrons.com/tags/blog/","section":"tags","tags":null,"title":"blog"},{"body":"I have been documenting so far with Blogger. Recently I started to do some Robotics projects with ROS. As I was doing it, I was looking for ways to get all what I do be documented. I know Github offers GH Pages, but I was a bit reluctant to try it out. So for the Navo project, I decided that GH Pages will be the source for my documentation, hence I wanted to give it a try. Honestly, I was not disappointed!\nI have been documenting all what I have been doing for the Navo project with GH Pages and a Jekyll theme. It did not click to my mind until a day ago that why not I move my personal blogging content as well from Blogger to GH Pages? I took the plunge and started to explore on what Static Site Generator or SSG in short platform should I consider. Jekyll is an option, but I wanted to see what other platforms are available.\nHugo, Gatsby, Octopress, Wordpress, Pelican, Zola were some platforms that I explored and decided to go with Zola. Well first, I liked the idea of documenting content for the web using Markdown files. The point here is Markdown is a text-to-HTML conversion tool for web writers, so why not just bother about writing the text and leave the conversion of text-to-HTML to some platform of choice.\nClearly there are a few advantages of using Markdown files for the content:\nNo need of a WYSIWYG editor\nNo HTML clutter in my content\nPlatform agnostic, yes that says it all\nVery lightweight and easy to get started\nThe most important factor for this migration is that I can focus on the content rather than the HTML formatting that I had to deal with in Blogger!\nSo my research began, the first SSG platform that I tried was Zola. Rust as a programming language occupies the same spot in my heart as Scala. Zola is built using Rust. Without any second thought, I decided to give Zola a try. It was the easiest thing to do in this world. Just 3 commands, and your static site is up and running. It took my about 20 minutes, yes, just under 20 minutes to try out Zola, choose a theme, get it up and running on my local machine.\nIt does not stop there, you could compile your Zola content to be served via GH Pages, which is what I wanted. I decided on a theme, thankful to this one here. I created a new project in my GitHub account, migrated all of my content from Blogger (this was a bit of work), organized them into appropriate folder structure.\nDoes it stop there? No, no no, it does not stop there, since your content is going to be in GitHub, you could apply that CI / CD pipeline. This was exactly my next step where I added a very basic HTTP link checker which when added to the GitHub action workflow could scan the Markdown files recursively and validate all the HTTP links.\nIt goes even further by automatically creating an issue with a summary of all the HTTP links that failed to resolve with a HTTP OK status. I do not need that as it makes sense to have such automation in a team set up. I could also set up a spell checker for my content as I realized during the migration of my content from Blogger is that I had lots of spelling mistakes / typo errors. Nevertheless, I did not want to overcomplicate this and decided not to add this action for spell check.\nAfter a successful HTTP check, the next step in the build pipeline is to compile the Zola source files into HTML's for rendering by GH Pages. There is a GitHub action exactly for this purpose, but I decided not to use it. All I need is the possibility to check out the content, using Zola compile it and commit the compiled sources into a branch under the same project. This branch will then contain the HTML's that will be integrated with the GH Pages for rendering.\nSetting up GH Pages, and linking it to a custom domain name is pretty straightforward. As of today, my content is not yet live as I'm still deciding on organizing my content. Hopefully in a few weeks time, bigelectrons.com will be served via Zola and GH Pages. {{ textcolor(color=\u0026quot;red\u0026quot; text=\u0026quot;EDIT: The website was taken live on 22.02.2021\u0026quot;) }}\nI will wrap this up here and for me it is Zola and GH Pages for the foreseeable future for all my blogging needs.\n","link":"https://bigelectrons.com/post/misc/blogger-to-gh-pages/","section":"post","tags":["blog","zola"],"title":"Moved from Blogger to GH Pages"},{"body":"","link":"https://bigelectrons.com/tags/distributed-systems/","section":"tags","tags":null,"title":"distributed systems"},{"body":"","link":"https://bigelectrons.com/tags/lamport-clock/","section":"tags","tags":null,"title":"lamport clock"},{"body":"Basically a distributed system is one in which the components or processes or nodes that comprise a system is distributed across nodes or sometimes even across geographies. But these systems need to communicate with each other to accomplish something meaningful. The most efficient, scalable and proven way of making these distributed systems communicate is via passing messages, sometimes even having a messaging middleware (like Kafka for example.,) as a central component.\nNow you can imagine when there is a very big carousel of such applications distributed across several nodes, where each communicate via passing messages, we need to somehow find a way to guarantee message ordering. Ok, yo we can rely on the system clock of the node? May be, but may be not. Here is why - System clocks struggle from Clock Skew and Clock Drift. On a distributed system, you cannot have something that acts like a global clock as that would one massively bring down scalability and increase your single point of failure. So a finesse solution would be to have something like a logical clock that each node in the application tracks and knows how to adapt its clock against its own message evolution and for the messages that it received from other nodes. One such algorithm is the so called Lamport Logical Clock\nI will try to simplify with my explanation on how this algorithm works.\nFor the sake of simplicity, let us assume that in our world of distributed systems, only 2 nodes exist. I call it Node N1 and Node N2. These Nodes care about the following Events:\nEvents that are generated internally in the Nodes - I call them Node Event or simply NE Events that are sent from one Node to the other - I call them Send Event or simply SE Events that are received on a Node from the other Node - I call them Receive Event or simply RE All nodes start with a Zero counter Lamport's logical clock states that each Node should have some form of incremental value (could be timestamp, or an integer) that can be associated with an event and could be compared against another value. So in this case, let us assume that we just have a simple integer counter for our Nodes and this counter will be incremented for every event that happens (be it NE, SE or RE), but how it is incremented depends on the following conditions:\nFor any NE, the counter will be incremented and, the new Event will have the incremented counter value\nFor any SE or RE, the following has to happen:\n1. The sender Node need to send the Event with the current counter value that the Event which is being sent has 2. The receiver Node, upon reception of the message, should then check for the counter value and should make sure it satisfies the following conditions: 2.1 The time of reception should be greater that the time of send 2.2 The receiver node should check against it's very own logical counter, and use the formula below to update the counter value in the newly received message new counter value = max(receive event counter++, local counter) So what happens at 2.2 is that as soon as a Node receives an Event, it immediately increments the counter value which is in the event and then checks / compares against its local counter value and whichever is the maximum, will then be this event's new counter value. The global counter in this node is then updated with this latest counter value.\nSo there you have it, without any pictorial representation, we have the Lamport Logical Clock unbundled!\n","link":"https://bigelectrons.com/post/engineering/lamport-logical-clock-1/","section":"post","tags":["lamport clock","distributed systems"],"title":"Lamport's Logical Clock - Ordering Events in a Distributed System"},{"body":"Arduino with L298N Let us see how we could integrate Arduino with the L298N which is our motor controller. Now, it you are unsure about why we need an L298N H bridge, here is some background information about this! With that being said, our sketch looks like this:\n1/* 2* DC Motor Encoder Test Sketch 3* by Joesan [https://github.com/joesan] 4* 27.12.2020 5* 6* Records encoder ticks for each wheel 7* and prints the number of ticks for 8* each encoder every 500ms 9* 10* WARNING: This is a throw away sketch, it is here 11* just for experimental purposes. 12* 13*/ 14// pins for the encoder inputs for Motor M1 15#define M1_ENCODER_A 3 // The yellow wire in the sketch above 16#define M1_ENCODER_B 4 // The green wire in the sketch above 17 18int enA = 8; 19int in1 = 9; 20int in2 = 10; 21 22// variable to record the number of encoder pulse 23volatile unsigned long m1Count = 0; 24double rpm = 0; 25unsigned long lastmillis = 0; 26 27void setup() { 28pinMode(M1_ENCODER_A, INPUT); 29pinMode(M1_ENCODER_B, INPUT); 30pinMode(enA, OUTPUT); 31pinMode(in1, OUTPUT); 32pinMode(in2, OUTPUT); 33 34// initialize hardware interrupt 35attachInterrupt(digitalPinToInterrupt(M1_ENCODER_A), m1EncoderEvent, RISING); 36 37Serial.begin(9600); 38} 39 40void loop() { 41// Turn on Motor A forward direction 42runForward(255); 43 44// Calculate the RPM every second 45if (millis() - lastmillis == 1000) { 46// Disable interrupt when calculating 47detachInterrupt(digitalPinToInterrupt(M1_ENCODER_A)); 48 49 // rpm = counts / second * seconds / minute * revolutions / count 50 rpm = m1count * 60 / 979.62; 51 Serial.print(\u0026#34;RPM =\\t\u0026#34;); 52 Serial.println(rpm); 53 m1Count = 0; 54 lastmillis = millis(); 55 56 // Enable interrupt 57 attachInterrupt(digitalPinToInterrupt(M1_ENCODER_A), m1EncoderEvent, RISING); 58} 59} 60 61void runForward(int speed) { //speed 0-255 62analogWrite(enA, speed); 63digitalWrite(in1, HIGH); 64digitalWrite(in2, LOW); 65} 66 67void runBackward(int speed) { //speed 0-255 68analogWrite(enA, speed); 69digitalWrite(in1, LOW); 70digitalWrite(in2, HIGH); 71} 72 73void m1EncoderEvent() { 74m1Count++; 75} 76 77// encoder event for the interrupt call 78/*void m1EncoderEvent() { 79if (digitalRead(M1_ENCODER_A) == HIGH) { 80if (digitalRead(M1_ENCODER_B) == LOW) { 81m1Count++; 82} else { 83m1Count--; 84} 85} else { 86if (digitalRead(M1_ENCODER_B) == LOW) { 87m1Count--; 88} else { 89m1Count++; 90} 91} */ 92} That piece of code above deserves some basic explanation. Let us try to do that!\nThe important part to note about the code above is the use of the attachInterrput function. Have a look here for a very good explanation of the function parameters and its meaning!\n","link":"https://bigelectrons.com/post/navo/arduino/arduino-with-l298n/","section":"post","tags":null,"title":"Arduino with L298N"},{"body":"","link":"https://bigelectrons.com/post/navo/","section":"post","tags":["ros","arduino"],"title":"Navo"},{"body":"DC Motor Speed Control with ROS We saw from the previous tutorial on how to measure the RPM of a DC Motor fitted with an encoder. In this tutorial, let us see how we could do a basic speed control using commands from a ROS node. Combined with what we learnt on how to use PID speed control algorithm let us put this all into practice. But first we need to create the circuit.\nOur hardware components are based on the Arduino Uno, the L298N H bridge for controlling the motors, a Raspberry Pi. We start with a brief overview about the L298N H bridge.\nL298N H Bridge The L298N is a DC Motor controller. The speed of the DC Motor could be controlled by regulating / changing the input voltage to the motor. This is done by using a technique called Pulse Width Modulation (PWM).\nOkay, so we can now control the speed of the motor with PWM, but what about the direction? Speed is not just about going forwards, but also going backwards. Fortunately this is very simple using a so called H bridge.\nHence, by combinig the PWM with an H Bridge, we can achieve what we want which is to control the speed and direction. The L298N is exactly this. I have purchased a few of these L298N Driver units. Each one is capable of controlling upto 2 DC Motors within the 5V and 35V range with a peak current of 2A.\nHere is a picture of my L298N H bridge. For the exact model, go here to find the list of materials that you need to source!\nNow that we know about the L298N Driver and its intended usgae, let us wire it up with the Arduino and see how we could control the motor.\nPutting it all together - Arduino, L298N \u0026amp; the DC Motor So for us, with this fundemental understanding, let us try this out by implementing a simple sketch and testing it out in the wild. Here is the fritzing schematic:\nI hope understanding the circuit is pretty straight forward. We just need to ensure that we have a common ground while the rest of the connections are self-explanatory. We just have connected a single motor to the circuit, though a single L298N H bridge can take 2 Motors. If we use the L298N H bridge in our Navo robot is a discussion for later as there are much better motor controllers which we will consider when we talk about the actual build.\nPower Supply\nAs it can also be seen from the schematic above is that, we are using a 12V external power supply from a battery pack. This is just about enough to also power the Arduino board.\nThe Logic We have already deduced how we could measure the RPM of the DC Motor that we are using. More information about the Arduino sketch can be found here. There is nothing ROS specific about that sketch. We will use that sketch as a starting point and make it respond to ROS messages from a ROS node, while at the same time, make that sketch send back messages to another ROS node with the RPM that the motor is currently running. Let's get our hands dirty!\nAdditionally, have a look here to understand how ROS works with the Arduino using ROS serial libraries. We use a PID control mechanism for doing the speed control, have a look here for a basic understanding of the PID controller.\nTODO.... Sketch and further documentation\n","link":"https://bigelectrons.com/post/navo/dcmotor/dc-motor-rpm-ros/","section":"post","tags":["dc-motor","ros"],"title":"DC Motor RPM Control"},{"body":"","link":"https://bigelectrons.com/tags/dc-motor/","section":"tags","tags":null,"title":"dc-motor"},{"body":"","link":"https://bigelectrons.com/tags/ros/","section":"tags","tags":null,"title":"ros"},{"body":"In this section, let us explore and understand this 17th century idea on how we could effectively do a speed control for the DC Motor, the PID Controller mechanism\nPID Control We will try to debunk a PID controller from a functional perspective and later on converge on the P, I and the D. The block diagram below shows the relation between a PID controller, and a process that is optimized by using a PID controller. The process here could be anything, but here we assume that the process here is a water heater control system (WHCS). The WHCS works by receiving a desired value for the temperature and heats the water. For example., if we want the WHCS to heat the water to 40° C, we want it to exactly do that.\nFrom the image above, we have the Process which emits a Process Value (PV), the PID Controller to which we give a Setpoint value (SP), the sensors that measure the actual Process Value (PV) which is then fed back into the PID controller, thus forming a closed loop. It can also be seen that the Process is also affected by external disturbances which makes it deviate from the Setpoint (SP). The PID controller's job is to account for these external disturbances and make the Process Value (PV) match the Setpoint (SP). For our case here where we want to do DC Motor speed control, we have something called a closed-loop controller, where we tell the controller how fast we want the motor to run. We call this the set point. The controller then measures the actual speed of the motor and calculates the difference between the actual speed, and the set point which is called the error. The controller then adjusts the voltage to the motor to reduce the error which in turn makes the Motor run at the set point we gave it originally.\nIf the SP and the PV are the same – then there is no other thing in this world that is going to be much happy than our PID controller. It does not have to do anything, it will set its output (the CV value from the image above) to zero, but in reality this is never going to be the case. So let us discuss further to understand the basics behind the PID controller.\nWe first need to understand what each of the term in the PID controller represent. The image below throws a bit of clarity on how each of the terms (Proportional, Integral \u0026amp; the Derivative) combine to get a smooth target output.\nAs we can see from the image above that the error is calculated by simply calculating the difference between the Setpoint (SV), and the Process Value (PV) or in other words, the error is simply the difference between the desired value and the actual value.\nLet us now try to mathematically understand what each of the terms mean and how they influence in minimizing the error.\nProportional Control This is the simplest of the control where the controller gives an output that is directly proportional to the current error (difference between SV and PV) times a proportional factor which is synonymous to the error value.\nIt can be seen from the equation above that with a small Kp the controller will make small attempts to minimize the error, while for a larger Kp, the controller will make a larger attempt to minimize the error. This is good, but this approach suffers from either an overshoot (if Kp is too large) or an undershoot (if Kp is too small). Both these effects, called the offset or the steady state error - we do want to avoid. There is always a steady state residual error in the case of a proportional controller. If this is too theoretical, let us understand it from a more practical perspective.\nImagive we have a water tank as shown in the image below with which our goal is to maintain a constant water level as defined by a Setpoint (SP) value. As long as the system is untouched, nothing changes but change is inevitable which means that the outflow will need to be increased, or the inflow need to be decreased or vice versa. But the goal remains constant which is to maintain the water levek in the tank at the given Setpoint value.\nSo with the goal of maintaining the Setpoint say at level 3, if we now increase the flow out of the tank, the tank level will start to decrease because of the imbalance between the inflow and outflow. While the tank level decreases, the error increases and the proportional controller will increase the controller output proportionally to this error. Consequently, the valve controlling the flow into the tank opens wider and more water flows into the tank.\nAs the water level continues to decrease, the error increases and valve continues to open until it gets to a point where the inflow again matches the outflow. At this point the tank level (and error) will remain constant. Because the error remains constant our proportinal controller will keep its output constant and the control valve will hold its position. The system now remains in balance, but the tank level remains below its set point. This remaining sustained error is called offset or the steady state error.\nTo get rid of this offset, the operator has to manually change the bias (the Kp term) such that the controller's output removes the offset. How can this be done automatically, that is exactly our next topic, the Integral controller!\nIntegral Control By introducing a time component to the proportional controller, we can mitigate the overshoot and undershoot problems, which we termed as the offset or the steady state error. The integral controller, is capable of handling errors that happen over a certain set time interval. With using an integral component, we can thus overcome the steady state residual error that occurs with the proportional controller.\nIf the error is large enough, the integral controller will increment / decrement the controller output at a faster rate, while on the other hand if the error is small, the changes will be slow. For any given error, the speed of the integral action is set by the controller's time setting where a larget time setting results in slow integral action and vice versa.\nThe equation above sums all the previous errors over a given time interval t and accounts for the error correction accordingly. If we have a larger value for the term Ki, it will result in an overshoot.\nDifferential Control or Derivative Control The differential controller or the derivative (slope) controller is effectively used to predict the systems future behavior. It is mostly used for processes where motion control is needed, hence a good candidate for our Navo robot. The proportional and the integral controller both respond to past errors, the differential controller is capable of predicting future behavior of the error. As it can be seen from the equation below that the output of the differential controller depends on the rate of change of error over time, multiplied by a derivative constant Kd.\nThe output of the derivative this helps in reducing / minimizing the overshoot error. For example., a larger derivative or a larger slope indicates that the next error will be far away from the previous error but a small derivative or a smaller slope indicates that the next error will be likely close to the previous error.\nPID Equation With the controllers and their equations nailed down, we have the following equations for the PID controller.\nand by substituting the values, we get\nSo what that equation basically mean in our context which is to control the speed of the DC motor is that, the controller output is used to determine how much more or less the motor speed has to be adjusted so that the current speed (processValue) matches the target speed (setPoint). How does all this look like in reality? This is what we show in the code below:\n1unsigned long lastTime; 2float processValue, output, setPoint; 3float errorSum, previousError; 4float kp, ki, kd; 5int interval = 1000; //1 sec 6 7void pidControl() 8{ 9 // Time interval to calculate the next error 10 unsigned long now = millis(); 11 float timeDiff = (float)(now - lastTime); 12 13 if(timeDiff \u0026gt;= interval) 14 { 15 // Calculate the error 16 float error = setPoint - processValue; 17 errorSum += error; 18 float dError = (error - previousError); 19 20 // Calculate the PID control output 21 output = kp * error + ki * errorSum + kd * dError; 22 23 // Memorize the variables for the next computation 24 previousError = error; 25 lastTime = now; 26 } 27} 28 29void gainTuning(float Kp, float Ki, float Kd) 30{ 31 kp = Kp; 32 ki = Ki; 33 kd = Kd; 34} As it can be seen from the sketch above that we calculate the output of the controller using the pidControl() function which is called at regular intervals. That piece of code above does not deserve any further explanation, but a few points are worth mentioning and reasoning about.\nDerivative kick This happens when a change in the value of the error happens suddenly as a result of a change in the Setpoint. This in turn causes the derivative of the error to be instantaneously large enouch so that we see small spikes in our process output. This is not a big deal, but for systems where the change in Setpoint occurs more often, it is better that we address this to avoid putting stress in the system as such. A simple idea to overcome such derivative kick is to assume that the change in the Setpoint is constant, so the rate of change or in mathematical terms, the derivative of a constant (remember derivative is all about slope and for a constant there is no slope) is 0.\nUsing what we already know for the derivative control, let us use our assumption for a constant Setpoint to the derivative equation\nAs it can be seen now that our derivative term becomes a measure of the Process value instead of the error term, hence our actual PID controller equation now becomes\nIt is easy to visualize this with some numbers and the resulting plat as shown in the image below.\nLooking at the plot between the derivative of the error vs time, It can be seen that as the Setpoint changes, it results in the error value to spike and this inturn causes the rate of change in error for a given time interval to result in a spike which we term the derivative kick. So by assuming that the Setpoint is a constant and derivating over the Process value actually helps to get rid of such a derivative kick.\nPID Gain Tuning Now one question might arise on what values to choose for the PID co-efficients. Luckily people have thought about this and the one that comes to mind is the Ziegler–Nichols method introduced by John G. Ziegler and Nathaniel B. Nichols in the 1940s. It is a heuristic technique of tuning a PID controller. The basic idea here is that it starts out by setting the integral and the derivative gains (basically the co-efficients Ki and Kd) to zero. The proportional gain (Kp) is then increased from zero until it reaches the ultimate gain Ku. This ultimate gain is the gain where the control loop has achieved a stable and consistent oscillation. The Ku and the oscillation period is then used to set the P, I and the D gains effectively. Let us not dive more into this for now.\nTuning the parameters or the gain in a running system is a topic in itself.\nHope I was able to explain the PID controller mechanism! With this basic understanding, head over here to see how we could combine all what we have learnt theoretically so far to a more practical example.\n","link":"https://bigelectrons.com/post/navo/dcmotor/pid-controller/","section":"post","tags":["pid-controller"],"title":"PID Controller"},{"body":"","link":"https://bigelectrons.com/tags/pid-controller/","section":"tags","tags":null,"title":"pid-controller"},{"body":"DC Motor A DC Motor equipped with a built-in encoder is going to immensely help us in precisely controlling the speed at which the motor rotates. Let us go through the basics behind such an encoder and how it relates to the speed ot the motor.\nIn this tutorial, we will look at how to do speed control using a DC Motor that has a quadrature shaft encoder. I will be using this DC Motor with Encoder as a reference. The main reason being that it has a very good specification documentation. The instructions here will be the same for any similiar motor with a quadrature encoder attached to the shaft of the motor.\nI will refuse to talk about DC Motors in general as that is not the focus for us, but rather to understand how to make use of the Encoder pulses with which we could measure the RPM and thus control the speed.\nUnderstanding DC Motor with Encoder So we need to now effectively understand a little about the DC Motor and the need for an Encoder unit. A one liner - a DC Motor is a mechanical device that converts basically electrical energy to mechanical energy. Ok that's simple enough but that's enough. Why do we need an Encoder? With the target that we are trying to build, we need to be able to more precisely control the speed and direction of the motor which effectively translates to the navigability of the Navo. Encoders transform mechanical motion into electrical pulses that can then be used by a controller unit (like the Arduino) to make adjustments and fine tune the motor speed and direction.\nAs mentioned before, we will use DC Motor with Encoder. The table and the image below clarifies the wiring definitions.\nColor Function Red motor power (connects to one motor terminal) Black motor power (connects to the other motor terminal) Green encoder GND Blue encoder Vcc (3.5 V to 20 V) Yellow encoder A output White encoder B output There are different types of encoders available, such as liner encoder or rotary encoders. A DC Motor encoder basically has a rotary encoder which is often times mounted to the shaft of the motor. The rotary encoder used here is a 6 pole magnetic disc attached to the shaft of the motor, along with two Hall effect sensors. When the motor turns, the 6 pole magnetic disk rotates past the two sensors and when each time a magnetic pole passes one of the sensor, the encoder outputs a digital pulse. So here with the two hall effect sensors, we get two output signals separated by 90 dgrees. The sketch below shows the basic working principle behind a Rotary magnetic encoder.\nIt can be seen that the two sensing units (marked A \u0026amp; B) will emit a pulse signal as soon as they hit the underlying magnetic pin (6 of them as can be seen in the diagram above) and this pulse signal can be translated / understood in terms of the direction and speed of the motor. I will not explain more about this topic, but here is a very descriptive video on understanding DC Motor encoders in much detail.\nCW -\u0026gt; Clockwise Direction CCW -\u0026gt; Counter-Clockwise Direction\nwe can now infer from the square wave pulse signals that binary combinations for the pulses A \u0026amp; B, we can infer that when we get a pulse signal that has a RISING for the pulse A and during this if we measure a LOW for pulse B, we know that the motor is in the forward direction (CW). If on the other hand when the pulse signal for A has a RISING and during this if we measure a HIGH for pulse B, we know that the motor is in the reverse direction (CCW).\nOk that's for the direction. Now let us see how we could leverage this information and with some additional data from the Motor's specification, determine the RPM which forms the basis for doing speed control. To do this we need to understand the motor's specification.\nThe DC Motor that we use has a metal gearbox with a ratio of 20.4:1 and a shaft with a diameter of 25 mm. The resolution of the encoder that is assembled to the motor shaft is rated at 48 Cycles Per Revolution (CPR). What this exactly means is that if the motor shaft turns one round the output of the encoder counts up to 48. It is important to notice that the gearbox shaft is different from the motor shaft and as mentioned before, the ratio of rotation between these shafts is 20.4:1. So if motor shaft turns 20.4 times, gearbox shaft turns just 1 time. So with this understanding, the cycles per revolution or pulses per revolution of the gearbox shaft can be calculated with the following formula:\nSo with this formula at hand, we get the gearbox CPR = 20.4 * 48 = 979.62\nThe gearbox output resolution (979.62 CPR) is the only value that we will get from DC motor as feedback which when coupled with time helps us do speed control. Now the formula to calculate the RPM (speed) is given by:\nwhere revolutions per count or revolutions per pulse equates to 1 / 979.62 (we know that pulse per revolution PPR or CPR is 979.62), so if we plug in these values into the equation above, we end up with the following:\nWe then have to plug in this equation into the sketch below and measure the number of pulse counts every second. Let us see how the sketch looks like!\nDC Motor Speed Control Arduino Sketch 1/* 2* DC Motor Encoder Test Sketch 3* by Joesan [https://github.com/joesan] 4* 27.12.2020 5* 6* Records encoder pulses and computes the rpm 7* every second 8* 9*/ 10// pins for the encoder inputs for Motor M1 11#define M1_ENCODER_A 3 // The yellow wire in the sketch above 12#define M1_ENCODER_B 4 // The white wire in the sketch above 13 14int enA = 8; 15int in1 = 9; 16int in2 = 10; 17 18// variable to record the number of encoder pulse 19volatile unsigned long m1Count = 0; 20double rpm = 0; 21unsigned long lastmillis = 0; 22 23void setup() { 24pinMode(M1_ENCODER_A, INPUT); 25pinMode(M1_ENCODER_B, INPUT); 26pinMode(enA, OUTPUT); 27pinMode(in1, OUTPUT); 28pinMode(in2, OUTPUT); 29 30// initialize hardware interrupt 31attachInterrupt(digitalPinToInterrupt(M1_ENCODER_A), m1EncoderEvent, RISING); 32 33Serial.begin(9600); 34} 35 36void loop() { 37// Turn on Motor A forward direction 38runForward(255); 39 40// Calculate the RPM every second 41if (millis() - lastmillis == 1000) { 42// Disable interrupt when calculating 43detachInterrupt(digitalPinToInterrupt(M1_ENCODER_A)); 44// rpm = counts / second * seconds / minute * revolutions / count 45rpm = m1count * 60 / 979.62; 46Serial.print(\u0026#34;RPM =\\t\u0026#34;); 47Serial.println(rpm); 48m1Count = 0; 49lastmillis = millis(); 50// Enable interrupt 51attachInterrupt(digitalPinToInterrupt(M1_ENCODER_A), m1EncoderEvent, RISING); 52} 53} 54 55void runForward(int speed) { //speed 0-255 56analogWrite(enA, speed); 57digitalWrite(in1, HIGH); 58digitalWrite(in2, LOW); 59} 60 61void runBackward(int speed) { //speed 0-255 62analogWrite(enA, speed); 63digitalWrite(in1, LOW); 64digitalWrite(in2, HIGH); 65} 66 67void m1EncoderEvent() { 68m1Count++; 69} The important part to note about the code above is the use of the attachInterrput function. Have a look here for a very good explanation of the function parameters and its meaning! Here is a very basic explanation behind using interrupts.\nHardware interrupts If you are a bit familiar with the pins on the standard Arduino board, the pins 2 and 3 are classified as pins for hardware interrupts. Have a look here to understand what interrupts are and how they can be useful. So in essence with hardware interrupts, your main program runs until the state of one of your interrupt pins change. The main program is stopped, a special interrupt method / function is called and then the main program resumes. For example., if your main program is about navigating your robot by avoiding obstacles, and if your robot recognizes an obstacle, the interrupt pin can be made to change its state from LOW to HIGH and this would trigger the interrupt function to be called, where you could then adjust the speed of the motor by slowing it down.\nFrom the sketch above, we have connected the Encoder A pin is connected to the hardware interrupt pin 3 of the Arduino.\n","link":"https://bigelectrons.com/post/navo/dcmotor/dc-motor-with-encoder/","section":"post","tags":["dc-motor"],"title":"DC Motor with Encoder"},{"body":"","link":"https://bigelectrons.com/tags/arduino/","section":"tags","tags":null,"title":"arduino"},{"body":"","link":"https://bigelectrons.com/tags/ci-cd/","section":"tags","tags":null,"title":"ci \u0026 cd"},{"body":"I have been a happy user of the Travis CI free usage over the last couple of years. But one announcement recently made me deeply worried. Have a look here at their announcement\nYes, Travis CI will no longer be free for OSS projects. There is no point in blaming Travis CI for this but rather on those idiots who abused the free usage with crypto minites and tor nodes. These bastards completely broke the trust Travis CI free tier gave for OSS projects. I loved their UI, and the way the builds are executed. I will no longer have the chance to use them. There is a free tier still available, but they are limited to 1000 minutes and this would mean that I can hardly do roughly 120 to 160 builds, and I'm not sure if this is spread over a year. But it is definitely not enough! Once they are exhausted, I probably need to beg for more. This is no fun!\n","link":"https://bigelectrons.com/post/infra/travis-ci-1/","section":"post","tags":["travis","ci \u0026 cd"],"title":"Deeply disappointed with Travis CI and the way OSS is treated"},{"body":"Pi with Arduino Let us see how we could set up the ROS serial communication between a Raspberry Pi and an Arduino board. So, our very first integration attempt is to integrate the Raspberry Pi with Arduino. We will use the Raspberry Pi 4. Let us break this down into the following steps:\nROS on the Raspberry Pi 4 Serial libraries on the Pi and Arduino Raspberry Pi 4 Setup It is pretty easy to get your hands on the Pi, so just go grab one and follow along.\nThe very first step with setting up your Pi is to install the base OS. You have many choices, but two of them stand out which is installing either Raspbian or Ubuntu. I prefer to run Ubuntu and ROS is very much compatible with Ubuntu, so why not!\nHead over here and make sure to download the Ubuntu server 20.04 64 bit LTS version (we will use ROS Noetic which is based on Ubuntu 20.04 LTS)\nAfter you have downloaded the image, head over here for a comprehensive installation instruction. It is pretty easy and straightforward!\nOnce you have your Raspberry Pi ready with Ubuntu 20.04, it is time to set up / install ROS! Since we have installed a headless Ubuntu, we need to first SSH into the Raspberry Pi. For this to work, you need to enable your Raspberry Pi network ready. Follow the instructions from here to make your Raspberry Pi visible to your home network.\nTODO....\nSerial libraries set up First let us setup Arduino. Now you could do this on your Raspberry Pi 4 or any other computer. You need to install the Arduino IDE.\nThe idea here is that we need to be able to make your Arduino serial communication capable and for this we have to install a few ros based libraries. There is nothing more specific to the Raspberry Pi 4. You could do the below steps from the Raspberry Pi 4, but assume that your Raspberry Pi 4 will be the server (probably running in your NAVO), and you do not want to have any IDE running on your production server, so I would set up my Arduino IDE on my Ubuntu machine.\nFollow the instructions from here to install Arduino IDE on Ubuntu. Make sure to download the appropriate Arduino IDE version for your Ubuntu version (For me it is Ubuntu 20.04). Get it installed as per the instructions!\nThe next step is to prepare and install the serial libraries that is required to make Arduino communicate with the Raspberry Pi. So for this, open a terminal window from the Ubuntu machine and run the following commands one after the other.\n1sudo apt-get install ros-noetic-rosserial-arduino 1sudo apt-get install ros-noetic-rosserial Make sure to replace the ROS version appropriately. For me it is Noetic!\nAfter this, fire up the Arduino IDE, by typing arduino from a terminal window (see the screenshot below)\nAfter this, open a new terminal window and cd into the arduino folder, and then cd into libraries directory. From that libraries' directory, run the following command below. This command should build the needed libraries that will be used by ROS.\n1joesan@joesan-InfinityBook-S-14-v5:~/Arduino/libraries$ rosrun rosserial_arduino make_libraries.py . If we now type the dir command, we should see the following:\n1joesan@joesan-InfinityBook-S-14-v5:~/Arduino/libraries$ dir 2readme.txt ros_lib As it can be seen that there is a new folder called ros_lib. That's all it with the setup. Let us now proceed to run some code in the Arduino. We will use some inbuilt program that will blink an LED upon receiving a message from a ROS publisher node.\nIntegration and test Fire up the Arduino IDE, and Go to File -\u0026gt; Examples -\u0026gt; ros_lib and open the Blink sketch program.\nUpload the code to Arduino. Make sure your Arduino is plugged into the USB port on your computer. To do this, either run Ctrl + U command from the Arduino IDE or Go to Sketch -\u0026gt; Upload which will then upload this program into your Arduino hardware.\nNow it is time to test our setup. Open a terminal window on the Ubuntu machine and issue roscore command. TODO: THIS SHOULD BE RASPBERRY PI!!!!!!!!\n1joesan@joesan-InfinityBook-S-14-v5:~$ roscore In another terminal window, launch the ROS serial server. More information about the ROS serial server can be found here\n1joesan@joesan-InfinityBook-S-14-v5:~$ rosrun rosserial_python serial_node.py /dev/ttyS0 Now let’s turn on the LED by publishing a single empty message to the /toggle_led topic. Open a new terminal window and type:\n1joesan@joesan-InfinityBook-S-14-v5:~$ rostopic pub toggle_led std_msgs/Empty --once On my machine, all the three commands above look like this. I'm using Tilix as my Terminal program. Note that additionally I'm tailing on the messages that gets published to the /toggle_led topic\nI made a small video to see this peanut setup in action! Click the image below for the video!\nThat's all it. To shutdown your Arduino, just disconnect it from your computer's USB port!\nKnown Issues While testing this setup, I faced some issues with the ros serial library:\n1In file included from /home/user/Arduino/libraries/Rosserial_Arduino_Library/src/std_msgs/Time.h:7:0, 2 from /home/user/Arduino/libraries/Rosserial_Arduino_Library/src/ros/node_handle.h:40, 3 from /home/user/Arduino/libraries/Rosserial_Arduino_Library/src/ros.h:38, 4 from /home/user/Arduino/libraries/Rosserial_Arduino_Library/examples/Blink/Blink.pde:6: 5 /home/joesan/Arduino/libraries/Rosserial_Arduino_Library/src/ros/msg.h:40:10: 6 fatal error: cstring: No such file or directory 7 #include \u0026lt;cstring\u0026gt; 8 ^~~~~~~~~ 9 compilation terminated. 10 exit status 1 11 Error compiling for board Arduino Uno. To resolve this, do the following:\nDelete the ros_lib folder which is to be found under /home/user/Arduino/libraries/ (here user is your logged-in username) Then open the Arduino IDE, in the menu bar go to Tools -\u0026gt; Manage Libraries and search for \u0026quot;rosserial\u0026quot; You should see Rosserial Arduino Library by Michael Furguson Make sure to install the 0.7.9 version Hope it made sense!\n","link":"https://bigelectrons.com/post/navo/arduino/ros-serial-arduino/","section":"post","tags":["arduino"],"title":"ROS Serial on Arduino"},{"body":"","link":"https://bigelectrons.com/tags/travis/","section":"tags","tags":null,"title":"travis"},{"body":"It is a good practice to write integration unit tests for your services and integrate them in your CI workflow. Such integration tests boosts the confidence of your application quality especially when it needs to be deployed in an environment where it has to talk to numerous other external services. In this regard, I wanted to write such integration tests for one of my scala project which uses SBT as a build tool.\nI will just list down the steps that are needed to get this up and running!\nFirst, in your build.sbt define the configuration and settings that can help sbt to differentiate between your integration unit tests and normal unit tests.\n1// Command to run integration tests, so here to run the integration tests we use -\u0026gt; sbt integration-test 2addCommandAlias(\u0026#34;integration-test\u0026#34;, \u0026#34;Integration/testOnly -- -n integrationTest\u0026#34;) 3 4lazy val IntegrationTest = config(\u0026#34;integration\u0026#34;).extend(Test) 5 6lazy val root = Project(base = file(\u0026#34;.\u0026#34;)) 7 .configs(IntegrationTest) 8 .settings( 9 IntegrationTest / parallelExecution := false, // We do not want Integration tests to execute parallely! 10 Test / testOptions += Tests.Argument(TestFrameworks.ScalaTest, \u0026#34;-l\u0026#34;, \u0026#34;integrationTest\u0026#34;), // Exclue the Inegration tests from the normal unit tests 11 // Include integration tests, by nullifying the above option 12 IntegrationTest / testOptions := Seq.empty, 13 ) 14 .settings( 15 // Enable integration tests 16 inConfig(IntegrationTest)(Defaults.testTasks) 17 ) We need to do a few more steps before we can get the whole thing up and running. We need to define an object that extends the Tag for your testing framework. So in my case I use scalatest and for me it would look like:\n1import org.scalatest.Tag 2object IntegrationTest extends Tag(\u0026#34;integrationTest\u0026#34;) After this, we can now start writing our test classes and tag them as IntegrationTest:\n1 test(\u0026#34;integration testing with sbt\u0026#34;, IntegrationTest) { 2 Future { 3 println(\u0026#34;ALLES OKAY *************************************** \u0026#34;) 4 assert(true) 5 } 6 } Now you can run your integration tests separately for your project as below:\n1sbt integration-test ","link":"https://bigelectrons.com/post/scala/scala-integration-test/","section":"post","tags":null,"title":"Mark \u0026 run integration unit tests with SBT and Scala"},{"body":"","link":"https://bigelectrons.com/tags/acl/","section":"tags","tags":null,"title":"acl"},{"body":"I could not say this more than what the title says. I had a very severe blow to my already damaged left knee. First a bit of history here and here.\nIt was on the 28th of September where I went for my first ever defensive sport training called \u0026quot;VingTsun\u0026quot;. Inspired by my Physio friend who is a beginner, I decided to give this training a try. I indeed watched few videos of it before committing myself to joining him for the training session which starts around 20:00 Hours near to where I live.\nConvinced that if during the training, should I need to use my legs a lot, I would talk to the trainer and avoid such moves, I happily started my session. Well guess, in the first 10 minutes where we were just warming up, I inflicted a severe blow to my left knee. What happened was, we were asked to run, run, stop, jump, run. Run was Okay, Run was Okay, Stop was Okay, but when I jumped (I guess it was the third time of my jump), and landed, my left knee buckled, and I fell down with such an excruciating pain. The pain was so intense that I could not breathe anymore, I was gasping for air.\nMy Physio friend tried to help out by pressing his hand against my knee, but that did not help. For the next 10 minutes I was just shouting out loud without being able to bear the pain. I got some ice applied, and I was just moved over to the corner of the room. As my home is a few kilometers away, and I was not in a position to travel back alone, I waited for the session to finish. My physio friend toed me on my bike back home.\nFast-forward a week, I'm back with my crutches. I think it is time for me to think about getting this damn thing operated. The pain was there for almost a week, and I still have the effect in my knee from the impact.\n","link":"https://bigelectrons.com/post/misc/acl-knee-injury-6/","section":"post","tags":["acl"],"title":"Once more a blow, a nasty one this time"},{"body":"Recently, I can say that I made very good progress playing across the neck of my guitar. I memorized few pentatonic shapes, but I was stuck the whole while on those specific boxes. The whole thing did not make any sense to me. Just wandering within those boxes is not pleasant music. Frustrated, I looked at Youtube for some motivational ideas on how to unstuck me from the boxes.\nLuckily, this one video in Youtube that I found on a random search helped me immensely.\n{{ youtube(id=\u0026quot;7xnBahYdWu8\u0026quot;) }}\nThe idea is to think in terms of triangles and move back and forth to those triangles. Man this was wonderful, and I now get the idea!\n","link":"https://bigelectrons.com/post/misc/guitar-learning-2/","section":"post","tags":["guitar"],"title":"On Navigating the Pentatonic scale"},{"body":"I have been using a Mac for over 7 years now, I have used about 3 Mac's where the 3rd one is still working fine. I have kids at home, they feed food to it and because of that the keys on my Mac is out of order. So it was time for me to have some privacy and get one machine for myself.\nLooking around the latest iteration of the MacBook Pro 13\u0026quot;, the configuration that I could get for the money and more importantly what I will be doing with it actually got me into thinking if I ever need a Mac at all? So I listed down my usual usage:\nBrowsing Software Engineering (Running application servers, Kubernetes, Docker images etc.,) Occasional photo editing Skype calls So given those needs above, I realized that a Mac is a bit pricey. So I started to search for alternatives. I could not tolerate a Windows machine, so the only option was to go for a Linux installed device. Fortunately, there are possibilities today to order one such Linux machine and run it out of the box. Yes, it is from Tuxedo computers. I purchased one of their business Laptops the TUXEDO InfinityBook S 14 v5. You can customize the hardware, I maxed out on the RAM as I know this is where my maximum needs are. It came fully pre-installed with their Ubuntu distro, they call it the \u0026quot;Tuxedo_OS 20.04 LTS 64 Bit\u0026quot; which is basically a Ubuntu Linux. Since it is LTS, it is guaranteed to receive updates for the next 5 years.\nIt took about 5 business days for the machine to reach me and I have it now with me. So far, I really do not miss my MacBook Pro. This machine is wonderful. What is astounding for me is the battery life. I get about 12 to 13 hours of battery life with some moderate usage (videos, browsing, terminal, docker containers etc.,). I'm pretty much satisfied with this as it just costed me 1/3 of the price for a similarly spec'd MacBook Pro.\nA quick review of the machine:\nThe chassis feels flimsy, not sure if it is truly military grade as mentioned in the website from Tuxedo\nThe Audio quality is poor, the video is kind of Okay. For the intended purpose of this machine, I'm fine with this!\nIt is very slender and light. I like that\nThe fan is silent and could not be heard under load\nBudgie desktop is pleasing to use and set up\nKeys have a nice reflex, and it is really inviting typing, but the keys could have been a bit bigger\n","link":"https://bigelectrons.com/post/linux/ubutnu-tuxedo-1/","section":"post","tags":["ubuntu","tuxedo notebook"],"title":"Moved from Apple to Infinitybook a.k.a Tuxedo a.k.a Ubuntu"},{"body":"","link":"https://bigelectrons.com/tags/tuxedo-notebook/","section":"tags","tags":null,"title":"tuxedo notebook"},{"body":"Recently I have been exploring more around the Kubernetes tooling, especially the ones that can do some pre-validation on the schema and the state of my Kubernetes deployment resources or more precisely the YAML files.\nI came across a few of them like conftest, kubeval etc and a wrapper around such set of tools like the kube-tools project which can be used as a GitHub Actions in your GitHub project.\nSo basically what I did was to try this out on one of my projects that I have on my GitHub, the plant-simulator-deployment which contains the Kuebrnetes resources to run the plant-simulator application in a Kubernetes cluster.\nSince I'm using GitOps for managing my deployments to the production cluster, I wanted to be sure that the YAML files that I write are indeed valid both in terms of the schema (Kubeval) and in terms of the state (Conftest). What I want is some kind of CI, test, a failure mechanism that would prevent any invalid YAML files getting pushed into the master branch because as soon as anything lands on the master, GitOops kicks in and deploys it immediately. So I need to be sure to fail such deployments in case any of my YAML file in invalid. How do I do this?\nSo here is basically my approach:\nUnder your project repo in GitHub, under Settings / Branches, create a new \u0026quot;Branch protection rule\u0026quot;. In our case, we want to protect the master branch as this is the one GitOps looks for!\nI create a status check to pass before the changes from a feature branch be merged into master (as seen in the screenshot below)\nTo test if this works, I intentionally introduced an error in one of my YAML files and you can see below that it was caught by GitHub Actions:\nNow I issue a pull request from this branch into master, you can now see from the screenshot below that the YAML validation failure shows up. So the guy who would do the merge / pull request would not merge it into the master! This is what we want!\n","link":"https://bigelectrons.com/post/infra/k8s-deployment-ci/","section":"post","tags":["k8s"],"title":"CI for Kubernetes Resources"},{"body":"","link":"https://bigelectrons.com/tags/k8s/","section":"tags","tags":null,"title":"k8s"},{"body":"I remember my interest to play the guitar started in the year 2012 and without any hesitation, I went and purchased my first ever electric guitar, the Ibanez GSA 60 and a small Marshall amplifier. I'm not sure at that point in time if it was the right decision to start with an electric guitar but as I recollect, I don't think I made any significant progress with it. I was even struggling to play the basic chord shapes. This deterred me from playing the guitar. The Ibanez mostly saw its life being on the shelf. Now and then I would just pick her up and try to learn something or the other, but never made any significant progress.\nFast-forward to 2015, my interest to learn the guitar got even stronger, I decided to take some private lessons. This was the time that I made two serious mistakes. One is to buy a very cheap beginner acoustic guitar and two to get some private lessons. I did those private lessons for about 6 months, and I realised it was not worth it. Maybe the teacher did not have any experience teaching beginners like me. The class, the lessons were not structured. No theory, no chords! It was pure waste of time and money. This experience again put me off in making any progress with my guitar learning!\nFast-forward to 2017, I again made an attempt to learn the chords, but before playing anything on the guitar, I watched tons and tons of videos and I realised that I should have a nice decent enough guitar that would invite me to play. After doing some research, I nailed down that I need to get the Seagull S6 Original. It was not available in stock here in Germany. I placed an order at the Music store in Köln sometime around September 2017 and waited and waited until it was almost a year, the answer from the store was that the stock did not arrive. When I ordered it, it was retailing for a price of about 419 Euros, but I got pissed off waiting for it and hence I went ahead and got my first ever proper acoustic guitar from Taylor, the Taylor 114e in November 2018 I guess. It was pricier than the Seagull, but in the end it was definitely worth it. Taylor 114e and Justin Guitar helped me to be comfortable playing most of the open chord shapes, change chords, strumming patterns, play a few songs. Overall I did make some considerable progress.\nI went back to Youtube, watched tons more of videos. I got interested in playing blues music. The more I watched those videos, the more I wanted to play blues. I decided that I need a quality electric guitar. I first tried to learn blues with my Ibanez GSA 60, but I somehow did not like the tone that came out of it and it was for me pretty limiting. Remember I'm not a complete beginner anymore. I would call myself as an advanced beginner. So I decided that I need some quality instrument that I can have forever. I sold the Ibanez and started to do some research.\nSometime around November 2019, I narrowed down to two of the famous iconic electric guitar models - The Les Paul and the Stratocaster. I went back to Youtube to understand the tonal differences between the two, watched tons and tons of videos, understood what the different pickups mean, humbucker vs single coil... and you know!\nI almost decided to get the Les Paul Studio, but after learning about the Fender Strat and making a few visits to the Music store in Köln and trying out both of these icons, I ended up getting the Fender Stratocaster Performer with the HSS configuration, yes in the sunburst color. No regrets, I made the right decision. I also got the Yamaha THR 10 V2 amplifier which made a killer combo with my Strat!\nI started to learn the pentatonic scale shapes, practiced them up and down and made myself comfortable playing it faster. But then I realised that I'm stuck within this one shape, so I wanted to understand how I could move left and right, up and down the neck which is what I'm learning right now. I did make some progress, but I still have a long long way to go and I have made playing the guitar a regular routine these days.\nSo my verdict is, if you are interested in learning to play the guitar, my suggestion is to go and get a decent enough instrument that makes you feel to pick it up and play. If you can afford it, I would strongly recommend to get an American made guitar, Fender it is!\n","link":"https://bigelectrons.com/post/misc/guitar-learning-1/","section":"post","tags":["guitar"],"title":"My journey thus far with learning to play the guitar"},{"body":"This morning it was just a normal day as always. We always have some balloons in our hall as my younger daughter likes to play it like a volleyball with me. I also enjoy entertaining her for some time.\nSo there were a few balloons in the hall, which I casually like to kick. As I kicked one with my right leg, my left leg kind of went out of balance, it twisted (as I have no cross ligaments because of my MTB accident 2 years ago. It was such a shocking sensation and such an intense pain that I could not stand anymore, I fell down ferociously!\nMy wife and my elder daughter who were nearby came rushing hearing the sound of my fall and my cries against the pain. For the next 5 to 10 minutes I felt such a pounding sensation on my left leg! It felt as though I had a severe accident! This is pure agony! Nevertheless, I guess it is time that I need to fix my left knee with an artificial ligament. I will probably consider operating it the next year or probably this winter after the MTB season this year ends. Until then, I'm gearing up for the summer to be on my MTB!\n","link":"https://bigelectrons.com/post/misc/acl-knee-injury-5/","section":"post","tags":["acl"],"title":"Oh my left knee - You cause my problems!"},{"body":"Some time around the summer of 2019, I came across a blog post where I read about GitOps and I was flattened by the idea itself and ever since I wanted to give it a try on some serious projects but never had the chance to do it professionally. I had a chat about this with many of my colleagues at work and my team, they seemed ro like the idea but were a bit reluctant to employ this concept in their projects. In this blog post I will attempt to explain what GitOps is all about and will probably give a reference to one of my private projects which I made GitOps ready!\nGitOps - As the name would say, use git for operations tasks. Simply put, you define your desired state of your infrastructure (let's limit ourselves to cloud native) and have this desired state pushed into your beloved git repos (BitBucket or GitLab or GitHub or whatever that supports Git version control). Then on the cloud side you will have your typical orchestrator (k8s or docker swarm) that will then wait for these changes that you make in the Git and once it sees the changes in your Git repo, it immediately pulls those changes and applies it on your production environment. It is as simple as that. This is exactly how k8s works. Isn't it. Just match the desired state on your production runtime.\nOk so now we know what GitOps is, let us see how it could be beneficial! We have been over the past couple of years talking about CI \u0026amp; CD pipelines, but I have seen from my experience that it is not CI \u0026amp; CD happening right after, but there is a huge gap between a CI and a CD. You know what I mean? You do CI more often than you do CD. So this would imply that you have to have perhaps additional tools to do CD work for your applications and more importantly the developer is no longer in charge of the CD process. It is left to some infrastructure operator (that classical ops guy or the notoriously famous site reliability guy for your cloud).\nHere are some really tangible benefits that GitOps (Pull-Based deployments) brings to the projects:\nCredential Management - How many times did you have to distribute your prod credentials into some other tool that is not your production environment? With GitOps, it is not going to be the case anymore. Your production credentials stays where it belongs - In your prod environment. This solves a very basic, but rather very important aspect for an organization - The Security! Yep! No more prod credentials on your developers machines or on your developer tools!\nRevert with confidence in case of problems - Since Git is versioning every bit of your repo, it makes reverting to an old desired state painless and effortless. It is as easy as saying git revert to any desired version or state you want your production to be.\nNow if you want to ship code more often to your production, this would mean that you should have a solid code base - One that is having a very good code coverage (unit testing, integration testing, e2e tests etc.,). When you employ GitOps, you commit yourself to deploy code more often and this would implicitly mean that you have to make your code base rigid with every subsequent release. I see the following implicit benefits:\nYou will improve your code quality (Given the fear factor that you might break something in production because of poor untested code when pushing more often to prod)\nYou will have confidence in your overall application landscape as you exactly know what's in Git is what is in your production.\nFrom my point of view GitOps is the way to go towards automising CD and keeping CD close to CI.\nI tried this on one of my project and its deployment project where I ran a Minikube cluster with the Flux operator installed on my Minikube cluster. It damn works! GitOps is the way to go!\n","link":"https://bigelectrons.com/post/infra/k8s-git-ops-1/","section":"post","tags":["k8s"],"title":"GitOps with K8s"},{"body":"","link":"https://bigelectrons.com/tags/neural-net/","section":"tags","tags":null,"title":"neural-net"},{"body":"I have been reading through the architectures of Neural Networks and wanted to grasp the idea behind calculating the weights in a Neural Network and as you can see in the image below is a simple 2 node 2 layer Neural Network.\nAs you can see that for simplicity’s sake, I have just used a 2 node 2 layer network, but the same holds true for any sized Neural Network. It's just that the size of the Matrix increases proportionally to the number of inputs! The concepts outlined in the image holds gold!\n","link":"https://bigelectrons.com/post/math/feed-forward-neural-net/","section":"post","tags":["neural-net"],"title":"Understanding Feed Forward Neural Network Architectures"},{"body":"","link":"https://bigelectrons.com/categories/math/","section":"categories","tags":null,"title":"math"},{"body":"I have been wondering on how the math behind a Linear regression works as in most of the ML books that you encounter, the focus will be on giving you a linear equation and just plugging this equation in a Python library to solve for the slope and bias and then use it to predict the new values. It is very rare that they show you how to find the m and b values. So here in a piece of paper, I decided to try that out, it worked out very well! So if you want to learn it, try to understand what partial derivatives are!\nIn the above solution, I have just solved for m, which is the slope term in a Linear Regression. You can apply the same technique to solve for b! So what you effectively do is to differentiate one term while treating the others as a constant. In simple terms, this is called a partial derivative. A derivative is a measure of something that changes while a partial derivative is a measure of something that changes while treating everything else in this world as a constant! It's that simple! Have a look here at the Wikipedia article on Partial Derivatives!\nWhat I have shown you is a Simple Linear Regression, but the technique applies as good as a multi variate Linear Regression! Math is fucking fun - Once you understand it!\n","link":"https://bigelectrons.com/post/math/linear-regression/","section":"post","tags":["ml"],"title":"Math behind simple Linear Regression"},{"body":"","link":"https://bigelectrons.com/tags/mtb/","section":"tags","tags":null,"title":"mtb"},{"body":"So as of today, I managed to clock on record a total of 1000 Km with my Trek Remedy 8. It is obviously much higher than that as for several rides I haven't turned my Sigma odometer device. But for the record, the mileage is 1000 Km for this year so far.\nI have explored much of the singles trails around my area! It was fun riding this bike! I hope to do even more miles for the next season!\n","link":"https://bigelectrons.com/post/misc/mtb-trek-remedy-1000-km/","section":"post","tags":["mtb"],"title":"My Trek Remedy clocks 1000 km this season"},{"body":"Last Thursday, I took a day off. As the weather was good I decided to take my kids and my mom to Apeldoorn in the Netherlands. We wanted to visit the theme park and spend the day there. My mom is here with us in Germany to visit us and to take care of the kids during their summer vacation. Apeldoorn is a 2-hour drive from where we live. As usual, we did not manage to start early and ended up reaching our destination around 12:00. By this time, the park was already crowded. Thinking that my mom could not walk for extended periods in the park, I reserved a battery operated wheel chain the day before - only to realise later that it was a big mistake!\nI reached the entrance, paid the entry, went and took possession of the battery operated wheelchair. To me it was simple to operate as explained by the service personnel at the park. My mom nodded her head as though she understood everything! She sat on it, drove for about 50 meters after which my elder daughter wanted to use the rest room. So I ordered my mom to park the wheelchair in a corner so that it does not hinder the pedestrians and other visitors to the park. She agreed to what I said and started to come towards one corner. I was standing with my younger daughter in my arms and while my mom was navigating the wheelchair to the corner, my elder daughter was trying to help her give directions on how to steer and corner. This somehow confused my mom, she gave full throttle and guess what, she whacked on my leg.\nAs I was standing sideways with my left side of my body facing the oncoming wheelchair, my mom was dashing against my left leg, I started to scream with pain and told her to stop which panicked her even further, she instead of taking her hands off the accelerator, she accelerated even further and gave couple more strikes with the wheelchair - the impact being taken by my already injured left leg. Since I do not have my ACL, my left leg would easily bend against this impact and this caused immense pain. Luckily for me the wheelchair came to a stop by dashing against an elevated floor, if not I had been probably fatally injured! All this while my younger daughter was in my arms. All this happened within a couple of seconds!\nThe impact of this freaky accident meant that I had to use my crutches for the next couple of days and also use a knee bandage like this one here to stabilise my knee. Subsequently, the pain kind of went away after a couple of days, but I missed out on those bike rides that I have originally planned! I will henceforth be extra cautious when out in such situations.\nSo after all this drama at the park, it was me who was using the wheelchair at the park and my mom was walking comfortably! I shouldn't have hired the wheelchair in the first place, I shouldn't have given it to my mom to ride it without her having enough practice! Lessons learned!\n","link":"https://bigelectrons.com/post/misc/acl-knee-injury-4/","section":"post","tags":["acl"],"title":"Yet another impact on the injured knee!"},{"body":"What does \u0026quot;Send it mean? It's a jargon used within the Mountain Biking community, especially the Downhill community that indicates that you are about to hit a jump and that you have to clear the jump! Jumpin on the trails requires technique, a lot of practice. One way to practice the technique is to do it using some artificially made ramps! That exactly was my DIY project this weekend! I went to the local store \u0026quot;Bauhaus\u0026quot; as it is called here in Germany and shopped for some wood and nails.\nThe dimensions for the ramp I wanted are {h = at least 30cm, l = at least 60cm}\nI made the choice that my ramp should be at least 30cm from ground so that I have some decent enough lift off! For this to happen, I had to buy a single piece of wood and have it cut into pieces! I would then assemble the pieces together using the nails! Here are some pictures!\n{% galleria() %} { \u0026quot;images\u0026quot;: [ { \u0026quot;src\u0026quot;: \u0026quot;/images/mtb/mtb-ramp-1.jpg\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;MTB Ramp 1\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;MTB Ramp 1\u0026quot; }, { \u0026quot;src\u0026quot;: \u0026quot;/images/mtb/mtb-ramp-2.jpg\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;MTB Ramp 2\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;MTB Ramp 2.\u0026quot; }, { \u0026quot;src\u0026quot;: \u0026quot;/images/mtb/mtb-ramp-3.jpg\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;MTB Ramp 3\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;MTB Ramp 3\u0026quot; }, { \u0026quot;src\u0026quot;: \u0026quot;/images/mtb/mtb-ramp-4.jpg\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;MTB Ramp 4\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;MTB Ramp 4\u0026quot; } ] } {% end %}\nIt was fun to build it and it roughly took me 40 minutes to get it done! It is not perfect, but for what it cost me (roughly 15 Euros), this is absolutely fine! I will probably test it the next days, I'm sure I'll have fun with it!\n","link":"https://bigelectrons.com/post/misc/mtb-ramp-1/","section":"post","tags":["mtb"],"title":"Send it... yep! Sendin it..."},{"body":"Today, I drove to Düsseldorf to Mountain bike the Aaper wald. I made a nasty mistake of not having enough breakfast. It started good, but after reaching the foot of the hill, I started to ride the up-hill section. After pedalling for about a minute or two, I started to notice that my legs were pounding hard. I feel that this must be the activity of my heart trying to pump more blood to my legs to feed them with power. After a while, the slope got even steep, I could not pedal it anymore, so I got down and started to tow my bike. It was demanding!\nOnce I reached the peak, I started to get tired and felt a pounding sensation on my face, I started to feel dizzy. It was within seconds, I blacked out!\nThe following event is as told by my friend who was with me for the ride!\nI fell down un-conscious, my bike fell over me. My fried then took the bike away and called me twice after which I got my consciousness back and got up! It was roughly less than 10 seconds where I went un-conscious! My friend told me that while I was un-conscious, my eyes were still open, he was not sure if I was breathing or not! I'm glad that I came back. I'm thankful to my friend that he called my name which I feel was the trigger to get me back to consciousness! After waking up, I felt as though I was sleeping for longer and while I was un-conscious, I was dreaming of something! Not sure what I was dreaming about. For about 20 to 30 seconds of waking up, I was still a bit dizzy, I was slowly getting back to normal. I had some water to drink and was just asking my friend about what happened to me.\nIt was after this incident, we drove through the forest for about 16km, both uphill and downhill where I was shredding the descents! I'm glad and thankful to my friend who offered me a sumptuous lunch with Salmon and some excellent coffee. It was a memorable day!\n","link":"https://bigelectrons.com/post/misc/mtb-ride-black-out/","section":"post","tags":["mtb"],"title":"How is it to experience a Blackout"},{"body":"Mountain biking is addictive! Worse than drugs!\nAfter a couple of months of staying away from my MTB, I'm back with it and I'm looking forward to this season to ride as much as possible and more importantly, without any accidents and injuries! I got my bike serviced and about a week ago. This is how \u0026quot;The Beast\u0026quot; looked after returning back from the service!\nI took it for a spin in the trails around Düsseldorf. I drove for about 30+ km and occasionally I had some strange feelings on my injured knee. The weather just got better that day. It was drizzling and the days before this, we had the storm \u0026quot;Eberhardt\u0026quot; that wrecked havoc near the area! As a result of all this, the trails were mushy and slippery.\nAt one point, when riding uphill, I got stuck between the roots and I could not pedal anymore as it was too steep. While I tried to put my left leg down to balance, I felt the buckling sensation! This is when I realised that I've got to be extra careful when riding technical trails and probably I will think of using a Knee protector / brace (like the Don Joy brace) that I had during the initial days of my ACL injury! But nevertheless, I continued and was able to complete the planned route. Overall I had a wonderful ride that day and I'm eagerly looking forward to discovering new trails and make new friends!\nI would like to share an interesting app called Komoot that I use to plan my MTB tours!\nKomoot is a social cycling app that you can use to plan your tours. Once you do so, you have the option to export your tour as a gpx data and import this into any navigation device. This would then give you a turn-by-turn navigation on your bike. I find this really helpful as I can now ride with confidence and just take any unknown terrain!\n","link":"https://bigelectrons.com/post/misc/mtb-ride-1/","section":"post","tags":["mtb"],"title":"Back with MTB Tours and Downhill rides"},{"body":"Joesan is my handle \u0026#x1f604; Here you will find some content, some more content, some thoughts, some experiments, some something that I do in my spare time. My memory is so volatile, so I thought it is a good idea for me to write them off somewhere and that somewhere is this blog! This is my blog! This is how I feed my curiosity! Have a read at some of my content!\nI'm on Mastodon\nFew Facts I'm into Enduro MTB \u0026#x1f6b4; I'm an amateur guitarist \u0026#x1f3b8; I'm fascinated by math \u0026#x1f44d; I'm magnetized to well-made / hand-made products \u0026#x1f44d; I'm learning through Coursera \u0026#x1f4d6; I'm currently working on the open-electrons project \u0026#x1f916; Get in touch My calender is clean \u0026#x1f4de; If you want to chat now \u0026#x1f4f1; Join the discussions \u0026#x260e;\u0026#xfe0f; Look here for Announcements \u0026#x1f3ba; Finally, a big thanks to this template based on which this website runs \u0026#x1f64f;\n","link":"https://bigelectrons.com/about/","section":"","tags":null,"title":"About Me"},{"body":"It is now roughly 4 months since I injured my knee and the recovery so far has been very positive. On the downside, I have the feeling that my knee is not like it was before! Occasionally I get a strange wobbly feeling on my injured knee!\nSo for the first 2 months, I was not able to fully stretch or bend my injured leg, but that got a lot better now. I can fully flex my left leg - as good as the right leg, but there is still a 10 to 20 degrees of deficit when bending it. I'm happy to have recovered to this extent. The last 2 weeks, I was back on my MTB - not yet on gnarly mountains, but on single trails. I was able to do a couple of rides stretching anywhere between 35 ad 40 km in a single sitting. I remember having a weird feeling on my left knee after the ride, but I believe it is just a one off thing that got eventually better after a couple of minutes!\nI'm now terribly confused. The 2019 season would start in just under 2 months from now. Getting a surgery is not an option now as it would put me off by 6 months using my bike. What I'm extremely afraid of are those jumps and drops and what effect would they have on my knee. Nevertheless, I don't want to be left without driving this season. I will avoid doing stunts and just drive for the fun of it! Hope to have a fun filled MTB season for 2019!\n","link":"https://bigelectrons.com/post/misc/acl-knee-injury-3/","section":"post","tags":["acl"],"title":"4 Months update of my Knee"},{"body":"","link":"https://bigelectrons.com/tags/mechanical-watch/","section":"tags","tags":null,"title":"mechanical watch"},{"body":"Mechanical watches - They are the representation of artistic ingenuity. If you are wondering what is so fascinating about mechanical watches, I welcome you to take a look at some Youtube videos that shows how a mechanical watch is manufactured. It is after knowing, understanding and appreciating the artwork that goes behind the making of a mechanical watch, I decided that I will get one for myself.\nI'm not a watch collector, I have never owned a time piece until I got my first mechanical watch in the spring of 2018. Here is a snap of that elegance!\nI'm going to talk about my NOMOS Tangente Reference 139. This comes with a sapphire crystal glass back so that you can stare at the inner body and feel the heart beat of the Tangente!\nA few terminologies first!\nThe movement or in horology terms, a caliber is the central part of any mechanical watch and is the only component that you find in your watch. The movement consists of the following components:\nPower Source - This is the hand screw that you wind your watch every morning Wheel Train - A set of gears that transmits the force of the power source to the escapement Escapement - Responsible for controlling the power source to the counting mechanism, the basis for counting seconds Oscillator - Also called as a balance wheel, oscillated back and forth at a constant time interval What is so special about NOMOS is that they have their own in house movement. Ok but what is so special about having an in house movement? Let's first define what an in house movement is - It is a movement that is designed, developed and assembled by the watch manufacturer that uses it for their watches. NOMOS in this context did exactly that! The Tangente has the Alpha movement from NOMOS.\nIf you have watched those videos about how a mechanical watch is made, you will realise how hard it is to compose a movement on your own! It requires a lot of time, research and investment. I've got to appreciate NOMOS for putting that effort in coming up an in house caliber. Mind it - it is not easy to come up with an in house movement for a company that is relatively young in terms of horologic timescales!\nThe NOMOS Tangente has this so-called Bauhaus design. Have a look here of what it represents! So to me this kind of Bauhaus design on a watch would mean simplicity and elegance. The Tangente stands true to this simplicity and elegance! There is not very much happening on this watch. Even lettered with a seconds dial located at the 6 '0' clock position. This is elegance in simplicity.\nSo if you are in the market for a simplistic mechanical timepiece, I would recommend having a look at the NOMOS Tangente. You could also get one with the date option! They look absolutely stunning!\n","link":"https://bigelectrons.com/post/misc/nomos-tangente-1/","section":"post","tags":["mechanical watch"],"title":"The beauty that goes behind a Mechanical watch"},{"body":"","link":"https://bigelectrons.com/tags/opencv/","section":"tags","tags":null,"title":"opencv"},{"body":"","link":"https://bigelectrons.com/tags/pir-sensor/","section":"tags","tags":null,"title":"pir-sensor"},{"body":"A few weeks after my terrible MTB accident where I tore my ACL on my left knee, I was off from work for a few weeks, and I kind of got bored with just laying in the bed and was pondering what I could be doing something meaningful. Then this idea stuck my mind, why not try doing a small CCTV camera for my home?\nI started to do a bit of research on what hardware I need. Picked up a Raspberry Pi, PIR sensor, a camera and started to assemble the components. Doing a CCTV camera using PIR is kind of old-fashioned, hence I decided to do it using OpenCV. I started to read some documentation about OpenCV and how I could do some face detection. Thankfully, the algorithms already exist, I just needed to plug in some Python code, train the model with some live samples and bang I have it.\nThe goal was to run the face recognition on the Pi and stream that into a computer attached to the network. The Pi monitors and if it detects any movement, it captures the face and can send the image to any destination where currently it is configured to send to Dropbox.\nThis was the challenge as I wanted the whole thing to run as a Docker container. If you could not follow what I mean, why not have a look at the project here\n","link":"https://bigelectrons.com/post/projects/project-raspi-motion-detection/","section":"post","tags":["pir-sensor","raspi","opencv"],"title":"Raspberry Pi Motion \u0026 Face Detection"},{"body":"","link":"https://bigelectrons.com/tags/raspi/","section":"tags","tags":null,"title":"raspi"},{"body":"Roughly 3 weeks ago on the 5th of October, I had an MTB bike accident and in the process I ended up injuring my left knee. I was actually attempting to Manual during which I realized that I was about to lose my balance. I wanted to bail out from the Manual, so I tried to pull the bike completely to the front with me jump landing behind the rear wheels, but the bailout got totally out of control and eventually I ended up twisting my left knee soon after the jump.\nIt was such a pain for the next 5 to 10 minutes. It kind of subsided, and I thought I could get back on my bike again (What a stupidity from me). As soon as I got on to my bike and tried to put my left leg down to balance the bike, I had a very sharp excruciating pain, and I could not balance any more with my left leg and fell down on the street. This is when I realized that there is something seriously wrong with my knee. I managed to limp, tow my bike back home. I was scared to think that I would have inflicted some fractures around my knee. Well, at that point in time I did not know anything about ligaments. All I knew was that I could have either fractured my knee bones or twisted my knee bone. How stupid! After about 2 hours, I went to a hospital that treats patients with emergency needs.\nI had an X-Ray which thankfully confirmed no fractures or whatsoever. Well, by this time I could not neither fully flex my left leg nor bend it completely. The doctor then administered some bandage and asked me to check with an Ortho as he was suspecting some injury to the ligament. Given that it was a Friday, I had to wait until Monday so that I could make some appointments with an Ortho nearby. Saturday and Sunday was when I did tons of research and educated myself about knee ligaments. After understanding ligament injuries, I had sort of mixed feelings - one a feeling that said, ligament injury is very common among sports, the other - it takes loads of time to recover completely (What an irony here - it just takes a couple of seconds to tear a ligament, it takes about a year to completely heal from it). Come Monday, I quickly managed to make an appointment with a nearby Ortho and he sent me for an MRI as he was suspecting ACL tear.\nFast forward 10 days, I had the MRI done and the results from the MRI centre was suggesting that I have a partial ACL tear, minor bone bruises, fluids build up in the joints, small baker cyst. No cartilage damage and no Meniscus damage. I took the MRI results back to my Ortho and he was just pushing me to get it surgically corrected. This was pretty strange as I asked him if I could try a Physio rehab first and then decide on surgery, but he was adamant and just blindly asked me to get a surgery done as soon as possible to fix my ACL. I was not happy about it, so I went and had a second and third opinion where in they told me to wait until the bone bruises go away and then I could still decide if I want the surgery or not! So roughly after about 3 weeks, I started with Physio sessions. I still do not have the full Range Of Motion (ROM) on my left leg, but everyday I see some very minor improvements - very minor though! Everyday I dream as though I'm walking normally without crutches. An ACL injury is something that definitely both physically and mentally affects you, but you should just take care that the injury does not take over your mind and that you are in control of it!\n","link":"https://bigelectrons.com/post/misc/acl-knee-injury-2/","section":"post","tags":["acl"],"title":"6 Weeks Update of my Knee"},{"body":"I wanted to play around with OpenCV and thought it might be a good idea to try OpenCV with a real life use case. DIY'ing a home camera system that can do motion detection and click images when there is some movement in the frame sounded like a cool idea. So I researched on how I could get this set up done.\nThere were quite a few things that I should decide, like for example., where will I mount such a camera - indoor or outdoors? If outdoor, then what about protecting the Pi from the harsh weather conditions? How will I mount the Pi such that it has a power supply? Until I answer these two questions, all what I do with my Pi is just going to be installed indoor. So I decided that I will mount my Pi in one of my rooms for the time being and just get to learn the software that is needed to get the job done. So it is going to be Python and OpenCV. I'm now spending considerable amount of time to familiarise myself with OpenCV. I have done Analytics before, but never had the chance to play with image recognition and processing. Intention is not to dive deep into how the Haar Cascade classifier in OpenCV works, but rather start with how to use it practically. Later on, I will probably get deeper insights into the algorithm itself!\nSo for now, the goal is to get the setup up and running! Here is what I have been up to so far\n","link":"https://bigelectrons.com/post/mlandai/raspi-with-opencv/","section":"post","tags":["ml"],"title":"CCTV Monitor with Raspberry Pi \u0026 OpenCV"},{"body":"Roughly 3 weeks ago on the 5th of October, I had an MTB bike accident and in the process I ended up injuring my left knee. I was actually attempting to Manual during which I realized that I was about to lose my balance. I wanted to bail out from the Manual, so I tried to pull the bike completely to the front with me jump landing behind the rear wheels, but the bailout got totally out of control and eventually I ended up twisting my left knee soon after the jump.\nIt was such a pain for the next 5 to 10 minutes. It kind of subsided, and I thought I could get back on my bike again (What a stupidity from me). As soon as I got on to my bike and tried to put my left leg down to balance the bike, I had a very sharp excruciating pain, I could not balance anymore with my left leg and fell down on the street. This is when I realized that there is something seriously wrong with my knee. I managed to limp, tow my bike back home. I was scared to think that I would have inflicted some fractures around my knee. Well, at that point in time I did not know anything about ligaments. All I knew was that I could have either fractured my knee bones or twisted my knee bone. How stupid! After about 2 hours, I went to a hospital that treats patients with emergency needs.\nI had an X-Ray which thankfully confirmed no fractures or whatsoever. Well, by this time I could neither fully flex my left leg nor bend it completely. The doctor then administered some bandage and asked me to check with an Ortho as he was suspecting some injury to the ligament. Given that it was a Friday, I had to wait until Monday so that I could make some appointments with an Ortho nearby. Saturday and Sunday was when I did tons of research and educated myself about knee ligaments. After understanding ligament injuries, I had sort of mixed feelings - one a feeling that said, ligament injury is very common among sports, the other - it takes shit loads of time to recover completely (What an irony here - it just takes a couple of seconds to tear a ligament, it takes about a year to completely heal from it). Come Monday, I quickly managed to make an appointment with a nearby Ortho , he sent me for an MRI as he was suspecting ACL tear.\nFast-forward 10 days, I had the MRI done, the results from the MRI centre was suggesting that I have a partial ACL tear, minor bone bruises, fluids build up in the joints, small baker cyst. No cartilage damage and no Meniscus damage. I took the MRI results back to my Ortho, he was just pushing me to get it surgically corrected. This was pretty strange as I asked him if I could try a Physio rehab first and then decide on surgery, but he was adamant and just blindly asked me to get a surgery done as soon as possible to fix my ACL. I was not happy about it, so I went and had a second and third opinion where in they told me to wait until the bone bruises go away and then I could still decide if I want the surgery or not!\nSo roughly after about 3 weeks, I started with Physio sessions. I still do not have the full Range Of Motion (ROM) on my left leg, but everyday I see some very minor improvements - very minor though! Every day, I dream as though I'm walking normally without crutches. An ACL injury is something that definitely both physically and mentally affects you, but you should just take care that the injury does not take over your mind and that you are in control of it!\n","link":"https://bigelectrons.com/post/misc/acl-knee-injury-1/","section":"post","tags":["acl"],"title":"What it is like to twist your knee?"},{"body":"There are times when we might want to call a function which calls itself repeatedly n number of times. In my career programming with Java so far, I have not written any single recursive functions, the main reason being that writing recursion was hard with Java. Java is best suited for an iterative approach. Sometimes a recursive code is a bit hard to read and follow. It is not that self intuitive as with an iteration.\nBehind the scenes, during each function call the local variables used within the function are placed on something called a call stack. The stack is unfortunately and fortunately limited in size and resources. If our recursive logic spirals out of control, we could very quickly fill up such a stack, deplete all your resources and bring our entire application down. We certainly want to avoid this from happening, so we write iterative logic which has a certain definite exit point, thus preventing us from any stack overflow errors.\nA functional language like Scala handles recursion much more elegantly and invites us to favour recursion instead of iteration.\nBroadly speaking, there are two kinds of recursion in Scala, the head recursion and the tail recursion. The main difference between the two is in the way Scala handles the function evaluation with the function call. In head recursion, the function makes a recursive call, the body of the function is evaluated using the result of the recursive call. In case of tail recursion, the function body is evaluated first, the result of the evaluation is used to make the recursive call. If there was no concept called tail recursion in Scala, we would have been still left to deal with stack overflows like we experience in Java, but tail recursion does some magic behind the scenes for us. Not so clear? Let's look at some practical examples:\n1def headRecursive(intList: List[Int]): Int = { 2 if (intList.length == 0) 0 3 else intList.head + headRecursive(intList.tail) 4} 5 6val sumOfInts = headRecursive(List(1,2,3,4,5,6,7,8,9)) // prints 45 Other than me defining the method name in the code snippet above to be called headRecursive, we can say that it is a head recursive function. If you take a deeper look into the method again, in the else block, to evaluate the line:\n1\u0026#34;intList.head + headRecursive(intList.tail)\u0026#34;, a call to the headRecursive(intList.tail) has to return. What happens effectively is that the recursive call is first made, then the head of the List is added to the result of the recursive call, the value is then finally returned.\nSo the runtime needs to know what was the value of the head element in the List, the result of the recursive call and it has to somehow store them into something called a call stack. So effectively, there is one call stack per recursive call. There is now an obvious reason to worry about stack overflows in case of a relatively huge List.\nLet's now understand how tail recursion helps us solve this problem with the same example tweaked a little:\n1@@scala.annotation.tailrec 2def tailRecursive(intList: List[Int], accumulator: Int): Int = { 3 if (intList.length == 0) accumulator 4 else tailRecursive(intList.tail, accumulator + intList.head) 5} 6 7val sumOfInts = tailRecursive(List(1,2,3,4,5,6,7,8,9), 0) // prints 45 The difference now is pretty straight forward, no not just in the method name, but the last call in the method is actually calling itself without any additional calculation which expects the method to first return the value and then perform the calculation. In the tail recursive case above, the sum is first evaluated (in the accumulator + intList.head) and then the recursive call is made.\nThis way, there is no need for the runtime to store the local variables during each recursive call in a call stack. The runtime can re-enter the same call stack again and again thereby eliminating completely a potential stack overflow error. Was that clear enough?\n","link":"https://bigelectrons.com/post/scala/scala-recursion/","section":"post","tags":null,"title":"Going behind Scala's recursion"},{"body":"Math Topic Source Simple Linear Regression Basics about doing Linear Regression Probability Bayes Theorem Basics about Bayes theorem in Probability Binomial Distribution Understanding basics behind Binomial distribution Doing Gradient Descent Understanding basics on Gradient Descent Essence of Calculus Series on understanding Calculus Category Theory Basics about Category Theory Useful Projects Title Source Grandslam 3D Mapping SLAM Mapping for Navigational Robots Monix Asynchronous programming for the JVM Technical Articles \u0026amp; Blogs Title Source IP Addressing Classful IP Addressing Basics SRE How Complex Systems Fail Miscellaneous Title Source Clutch yourself in a Trance - 1 Mahaavatar Babaji - 1 Clutch yourself in a Trance - 2 Mahaavatar Babaji - 2 Markdown Syntax Reference Title Source Markdown Syntax Reference Reference on how to format the Markdown content Zola Extended Shortcodes Reference on how to format the Markdown Shortcodes Useful Markdown Emoji's All Emoji References for Markdown ","link":"https://bigelectrons.com/post/links/tech/tech-links/","section":"post","tags":null,"title":"List of useful tech links"},{"body":"","link":"https://bigelectrons.com/post/links/","section":"post","tags":["index"],"title":"Summary of useful links"},{"body":"Math \u0026amp; ML Title Source Deep Learning Deep Learning MIT Press Neural Networks Make Your Own Neural Network Statistics Basics Head First Statistics Statistics for ML Introduction to Statistical Learning Programming Topic Source Algorithms Grokking Algorithms C++ C++ Primer Go Effective Go Kubernetes Kubernetes Patterns Rust Rust Programming Scala Functional Programming in Scala - The Red Book Misc Topic Source Electricity Living on the Grid Investing The Intelligent Investor Memoir Autobiography of a Yogi Thoughtful The Innovators Dilemma ","link":"https://bigelectrons.com/post/links/books/books-links/","section":"post","tags":null,"title":"Lists of useful books"},{"body":"Some time ago I managed to set up a 4 node K8s cluster on a set of Raspberry Pi's that were lying idle at my home. In case it interests you, please have a look here for the complete setup, the required components and on how to get it up and running\n","link":"https://bigelectrons.com/post/infra/k8s-raspi-cluster/","section":"post","tags":["k8s"],"title":"K8s Cluster SetUp on a Set of Raspberry Pi"},{"body":"blockchain-documentation This is a summary of the concepts around Blockchain technology!\nThere are two types of Software architectures exists - Centralized \u0026amp; Distributed A Hybrid architecture is also possible Centrally Distributed architectures Blockchain simply put is a tool to maintain Trust \u0026amp; Integrity in a peer-to-peer system The challenge is to solve the Trust \u0026amp; Integrity in the worst of all situations The Blockchain challenge - The idea behind Mining or Proof-Of-Work: Here is a very simplistic view of what the actual miners do when mining for the Bitcoin, or rather to put it in mathematical terms, solving a puzzle - The puzzle is always to find the leading zeros in the resulting hash of a new block. For example., take a look at the following function written in Scala:\n1def sha256Hash(text: String) : String = String.format(\u0026#34;%064x\u0026#34;, new java.math.BigInteger(1, java.security.MessageDigest.getInstance(\u0026#34;SHA-256\u0026#34;).digest(text.getBytes(\u0026#34;UTF-8\u0026#34;)))) 2 3@scala.annotation.tailrec 4def mineSomeShit(str: String, appender: String, difficulty: String): String = { 5 val hashed = sha256Hash(str) 6 println(hashed) 7 if (hashed.take(difficulty.length).startsWith(difficulty)) 8 hashed 9 else { 10 val newString = str + appender 11 (mineSomeShit(newString, appender + appender, difficulty)) 12 } 13} We will hash the String \u0026quot;Hello\u0026quot; and see if the resulting hash contains a leading zero's (the difficulty we set). If we do not find the hash with the expected number of leading zeros (the difficulty we set) we will append the String \u0026quot;Hello\u0026quot; with 1 to the end and re-compute the hash and check if we have the needed number of leading zeros. So here our appender that we pass to the mineSomeShit method is the nonce!\n1mineSomeShit(\u0026#34;Hello\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;0\u0026#34;) See I'm setting the difficulty to single zero which is what I expect to see at the staring position of the resulting hash. If I find a match from the resulting hash, I return, but if not, I keep appending a 1 to the end of \u0026quot;Hello\u0026quot; and continue hashing the resulting new String. This way of adding arbitrary String in our case \u0026quot;1\u0026quot; in Bitcoin terms is called a nonce! So a test run on my Mac would look this:\n1scala\u0026gt; mineSomeShit(\u0026#34;Hello\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;0\u0026#34;) 2185f8db32271fe25f561a6fc938b2e264306ec304eda518007d1764826381969 3948edbe7ede5aa7423476ae29dcd7d61e7711a071aea0d83698377effa896525 4b11cd38a7f952ff52348c60fa6436ad041f4273a6ad43200b48747ff7aae8557 57c01a6b99ca7d331c85b2f6826cf5c6429613d617ced1a761e36298de903c1a5 63a914c051d4c6902b83b086ad982148a3213e0c3d3d29d46027b7d118f7fc599 79e77cc0f3906d514f79889ec8d49b94488f82178fb368fef286b26f3964aa077 88e8a20333f0fc59553b7a14a269206688508e653f5058e891711eba61cd5df17 93740a40fb9b1f71f6e69b8268335aaf451b077c0ffe0df94d46c229175f21a16 10+ 0xc547c99864a134db0c95e459b885e34b5e3ecd70f134e574c593e7fb113ef3 So there we go! We found out the hash with a single leading zero - I solved this puzzle - I get a ShitCoin for my \u0026quot;Proof of Work\u0026quot;. Let us now notch it up a little by setting the difficulty to finding 2 leading zeros in our resulting hash. A test run on my Mac is as below:\n1scala\u0026gt; mineSomeShit(\u0026#34;Hello\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;00\u0026#34;) // Setting the difficulty to two leading zeros 2185f8db32271fe25f561a6fc938b2e264306ec304eda518007d1764826381969 3948edbe7ede5aa7423476ae29dcd7d61e7711a071aea0d83698377effa896525 4b11cd38a7f952ff52348c60fa6436ad041f4273a6ad43200b48747ff7aae8557 57c01a6b99ca7d331c85b2f6826cf5c6429613d617ced1a761e36298de903c1a5 63a914c051d4c6902b83b086ad982148a3213e0c3d3d29d46027b7d118f7fc599 79e77cc0f3906d514f79889ec8d49b94488f82178fb368fef286b26f3964aa077 88e8a20333f0fc59553b7a14a269206688508e653f5058e891711eba61cd5df17 93740a40fb9b1f71f6e69b8268335aaf451b077c0ffe0df94d46c229175f21a16 10+ 00c547c99864a134db0c95e459b885e34b5e3ecd70f134e574c593e7fb113ef3 Ok! I get another ShitCoin - Glad that I can do this with my Mac! Let us notch it up even higher, this time around, the puzzle to solve is to find a hash with 3 leading zeros - Guess what, my Mac could not handle it, the JVM could not handle it! Here it is:\n1scala\u0026gt; mineSomeShit(\u0026#34;Hello\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;000\u0026#34;) // I\u0026#39;m setting the difficulty to 3 leading zeros 2185f8db32271fe25f561a6fc938b2e264306ec304eda518007d1764826381969 3948edbe7ede5aa7423476ae29dcd7d61e7711a071aea0d83698377effa896525 4b11cd38a7f952ff52348c60fa6436ad041f4273a6ad43200b48747ff7aae8557 57c01a6b99ca7d331c85b2f6826cf5c6429613d617ced1a761e36298de903c1a5 63a914c051d4c6902b83b086ad982148a3213e0c3d3d29d46027b7d118f7fc599 79e77cc0f3906d514f79889ec8d49b94488f82178fb368fef286b26f3964aa077 88e8a20333f0fc59553b7a14a269206688508e653f5058e891711eba61cd5df17 93740a40fb9b1f71f6e69b8268335aaf451b077c0ffe0df94d46c229175f21a16 1000c547c99864a134db0c95e459b885e34b5e3ecd70f134e574c593e7fb113ef3 11e8cb73f4c56f07fe838d6ad0e4e659b79542d2db4b31895f2592986c71d7b235 12fe6b9a3f791c9b5b8afc66b7d9975581e197575de38bb51e354dd3f537e58796 13d94210fea07e7b95235e6a544338fa536751db1a941ee6659915e5ec8ae4c23f 141e09d7ac0a3f950d8a244ec50362a52c2d8b4f97b2d6703cf2af787f8836d82c 15158727d24e0a72ed6a28b405232466eff0dcfafc8975be03a7fd5bcad9b6ea02 166f1ff66ba394c3c632a7e552846d9cdd14a57ad19d117a98272a4188ee805e9a 17f56f777268162d1a149a69fa8aab070ee00e2215b3fea2123c016ea997185632 183b5b867d31ab7daf63c0a66eefd86a5cc0801b5044f2a21bd50f2e1cee671ec4 19ace085f11c9d6a1bc36d4292a3c336d007df223e1adebdd65b6342c6a56d7b2a 20ac4a86fcd4e85554a0dadd15d832bd97cd7d5d5e3bcdf3d843ee7a5867bd8c0a 21e69f3899145b9e4c40802235183ea90f3cb21051fabd05dfe341166a2f4fdb4c 227c16a5ed55c6bda49bf5ef8cd72cfa45f24d4189ee987d36973642a1dbebaffe 2362cd5b96ffad643ed45caa2020eb1bc9dd4719bb9743fda4049198d9f5ec6db6 2493405577cf3300f06a36ef59aa0201b9aa391f6badb55871313ad9fc57d91aec 2572d47bce4c4b90e3a2f9a5d29a2acb377aa5d18c4da1c611fd934764e7ba2c0d 26java.lang.OutOfMemoryError: Java heap space This is exactly what happens when mining BitCoins. You need computational power as the difficulty goes higher and higher with every batch of Blocks! There is no other way to find the hash with leading zeros, other than doing a brute force trial and error by adjusting a portion of the input block and calculating hashes over and over again until one of the hashes fulfills the puzzle by chance. Well, finding a hash with leading zero's is one thing while setting a target on the found hash with leadin zeros is what makes the mining even harder!\nRoughly every 14 days the Bitcoin difficulty is adjusted such that the time between successive blocks remains constant at 10 minutes.\nThe screenshot below shows the latest block info (as of 3-Nov-2017) from the Bitcoin network:\nThe puzzle is to find the hash with 18 leading zeros as it can be seen in the Hash! So you can imagine now why a Million dollar is needed to solve a single puzzle!\nIf you then look at the following URL, you can figure out the latest BlockInfo and from there you can figure out the difficulty that is current when mining a block!\nhttps://blockchain.info/block\n500 peta hashes per second is produced by the Blockchain network. The average of 10 minutes (time taken to mine a single block) is maintained dynamically by the Blockchain network!\nSHA256 is just about flipping bits - flipping bits need energy - heat - with the heat generated from this hardware, I can heat my household, toast a bread, this hardware is specifically designed to do hashes - If you do not cool them they melt! - Doing this in Chennai, my hometown which never has Winter, would not be economical!\nWhile at it, I wanted to create a simple private ethereum network, and I did manage to do it and run them as Docker container's. I just ran two nodes on my Mac and I never saw my battery draining so fast! These Blockchain networks are certainly power thirsty! Take a look here for the Demo: https://github.com/joesan/lab-chain\nApart from this, if you can grasp the ideas behind the following topics, you have understood somewhat technically what Blockchain is and how it works in the Bitcoin setup.\nTransactions Difficulty Hash Nonce Double Spending Problem Byzantine General's problem You can find more details on the above mentioned topics at this page: https://en.bitcoin.it/wiki/Main_Page\n","link":"https://bigelectrons.com/post/crypto/blockchain-documentation/","section":"post","tags":["crypto"],"title":"Basics on Blockchain Mining"},{"body":"","link":"https://bigelectrons.com/tags/crypto/","section":"tags","tags":null,"title":"crypto"},{"body":"","link":"https://bigelectrons.com/tags/akka/","section":"tags","tags":null,"title":"akka"},{"body":"","link":"https://bigelectrons.com/tags/monix/","section":"tags","tags":null,"title":"monix"},{"body":"If you are in the Energy \u0026amp; utilities industry, you might have probably heard about a concept called Virtual Power Plant.\nThe idea is not new, but it is gaining importance as more and more power producing units (also power consuming units) gets distributed. What does it mean? It just means that the internet offers the possibility to command and control remote power units. The way it works is relatively simple. One of the main uses for such a virtual power plant is to provide flexibility services to the grid so that the grid is stable.\nNormally, the grid works by predicting the demand ahead, but as you know it is just a prediction from a historical value which means there is going to be a delta plus or minus and this is not good for the grid. The grid has to be operated at it's set frequency otherwise there will be a blackout. This is exactly where the virtual power plant's can be used.\nThe core of a virtual power plant is the software which aggregates smaller power producing units and virtually projects it as a big power plant which can then be integrated to the grid. The grid operators are given this aggregated capacity (often called as flexibility) and can be used to offset grid imbalances.\nSo with that basic understanding of the virtual power plant, I created a simple simulator (a.k.a. Virtual Power Plant or in general Digital Twin)that can aggregate and steer such power plants. It is not some system that does some random simulation, but a real production ready code base that can take a beating. It is built truly on the principles of reactive programming. I tried to incorporate all the best practices that could be attributed. For example., the application has the following features:\nIt is being kept updated for its dependencies using a bot It is capable of working with any relational database, just change the DB configuration (Oracle, Postgress or MySQL) It is extremely scalable It has in-built alerting \u0026amp; monitoring with no additional dependency on any external library It has a very good unit test coverage It has an automated deployment mechanism (using GitOps) It has deployment files that are run through CI / CD steps It has streaming API's It has back-pressure where necessary On top of all that, it is built using Scala, Akka \u0026amp; Monix. Who needs Kafka? Have a look here for the project\n","link":"https://bigelectrons.com/post/projects/project-plant-simulator/","section":"post","tags":["vpp","akka","monix"],"title":"Plant-Simulator"},{"body":"","link":"https://bigelectrons.com/tags/vpp/","section":"tags","tags":null,"title":"vpp"},{"body":"If you tell me that in your programming career, you have never been inflicted at-least once with a NullPointerException, you were simply the best programmer out there. During the early days of my career. I can remember times and times again, I ended up hitting NullPointerException's until I really made it a point to adjust my design to completely get rid of them. It was just me trying to get rid of them, but the NullPointers weren't prepared to leave me. Alright, let's look at it in steps!\nI have a method called myMethod that returns me a String value after doing some lengthy calculation:\n1def myMethod: String = { 2 ... 3 val retVal = doLengthyCalculation() 4 ... 5 retVal 6} Now imagine that the myMethod is called by another method which would do something with the String value it gets.\n1def anotherMethod: String = { 2 ... 3 myMethod.equals(\u0026#34;someValue\u0026#34;) // Potential source for NullPointerException 4 ... 5} From the above code snippet, it is obvious that myMethod could return a null value and when the caller tries to invoke a method on a null value... I don't want to say what happens next! You know it! So how does Scala deals with this? Scala, since being a functional programming language, has a concept called Monad (borrowed from Category Theory). I won't go through what a monad it, but for a one-liner, here is what it is:\nA monad is sort of a container of a certain type. You can unpack the container, do some operation on the type, pack the container back and keep doing this operation over and over again.\nI know you might be scolding me as that definition did not make much sense. Yes I know that! For a detailed look into what Monads are have a look at this wikipedia article\nNow coming back to our code snippet, the method myMethod said to the external world that it is going to return a String but what happened eventually in our scenario is that it returned null which is a pure violation of the contract.\nScala solves this elegantly by introducing something called an Option. Now when I apply this Option as a return type to myMethod, it becomes:\n1def myMethod: Option[String] = { 2 ... 3 val retVal = doLengthyCalculation() 4 ... 5 retVal 6} The myMethod contract now says that, I may or may not return a String, so whoever calls me, be prepared to handle this. It is a lot better to know this beforehand than to wait until Runtime to beconfronted with a NullPointerException. The caller of myMethod would then handle this as described in the code snippet below:\n1def anotherMethod: String = { 2 ... 3 myMethod match { 4 case Some(strValue) =\u0026gt; // Hurray no more NullPointerException\u0026#39;s 5 case None =\u0026gt; // You know what to do 6 } 7 ... 8} Together with the Scala's pattern matching syntax, the Option types are an elegant way of dealing and avoiding NullPointerExceptions.\n","link":"https://bigelectrons.com/post/scala/scala-null-pointer/","section":"post","tags":null,"title":"Void NullPointerException with Scala Option"},{"body":"I will not debate upon whether writing private methods are a code smell or not but rather just show how a private method can be unit tested using the Scala Test framework\nHere is our test class where we would write a unit test which tests a private method called myPrivateMethod in a class called MyClass\n1class MyPrivateTester extends FlatSpec with Matchers with PrivateMethodTester { 2 \u0026#34;PrivateMethodTester\u0026#34; should \u0026#34;be able to test a private method in MyClass\u0026#34; in { 3 val privateMethod = PrivateMethod[String](\u0026#39;myPrivateMethod) 4 val myClass = new MyClass 5 val invocationResult: String = myClass.invokePrivate(privateMethod(\u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;)) 6 } 7} A few points to note about the test code is that we need to extend the PrivateMethodTester trait. The signature for a private method invocation looks like:\n1val privateMethod = PrivateMethod[ReturyType_Of_The_Private_Method_That_Is_Tested](\u0026#39;Private_Method_Name\u0026#39;) The type PrivateMethod is a type member of the PrivateMethodTester trait. We give it the return type of what the private method is returning together with the private method name.\nOnce we have the PrivateMethod defined, we can use it to actually call our private method on the object or class type that we want ot test. In our example above, we wanted to test the private method named myPrivateMethod available in the MyClass object. Assuming that myPrivateMethod takes two String arguments, the call to test this method would look like:\n1val invocationResult: String = myClass.invokePrivate(privateMethod(\u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;)) The invocationResult is the result of calling the private method.\n","link":"https://bigelectrons.com/post/scala/scala-test-private-method/","section":"post","tags":null,"title":"Test private methods with Scala Test"},{"body":"One of the coolest features since Scala 2.10.3 are the Scala dynamic types. Let's look at what they are!\nA dynamic type is a type with which we can dynamically add fields / methods to an existing type. This is better explained with some examples\nAssume that we have a scala class as defined below:\n1 class MyClass { 2 def myMethod(str: String) = println(str) 3 } 4 ... 5 ... 6 val myClass1 = new MyClass().myMethod(\u0026#34;printme\u0026#34;) // Fine! 7 val myClass2 = new MyClass().notMyMethod(\u0026#34;beep\u0026#34;) // Yes you know it! For situations like above, scala dynamic types come to the rescue. They are a mechanism by which we can intercept calls to a non-existing field or a method in a class\nLet's now modify our MyClass and try to get rind of the compile error when invoking the notMyMethod!\n1 import scala.language.dynamics 2 3 object MyClass extends Dynamic { 4 5 private var myMap = mutable.Map[String, Any]() 6 7 def myMethod(str: String) = println(str) 8 9 def selectDynamic(args: String) = println(\u0026#34;selectDynamic \u0026#34; + args) 10 11 def applyDynamic(methodName: String)(args: Any*) = println(\u0026#34;applyDynamic \u0026#34; + args) 12 13 def applyDynamicNamed(methodName: String)(args: (String, Any)*) = println(\u0026#34;applyDynamicNamed \u0026#34; + args) 14 15 def updateDynamic(name: String)(value: Any) = { myMap(name) = value } // mutating sounds scary!!! 16 } Let's go through each one of those xxxDynamic methods that we added in the example above:\napplyDynamic - Dynamically creates a method and the arguments as though that method was part of the declared type applyDynamicNamed - Similar to applyDynamic, with the benefit that we can use a named argument selectDynamic - Dynamically invoke a field as though that field was part of the given type (think of a getter) updateDynamic - Dynamically update a field as though that field was part of the given type - (think of a setter) I wonder why there is the updateDynamic, I would rather refrain from using it for obvious reasons that we all know (avoid mutability damn it!)\nTry the following examples and figure out which xxxDynamic method is invoked in each case\n1MyClass.showMyAge(\u0026#34;my age is\u0026#34;, 34) - ??? 2MyClass.FUCK!! - ??? 3MyClass.printUser(userName = \u0026#34;Joe\u0026#34;, age = \u0026#34;34\u0026#34;) - ??? 4MyClass If you have got all the above examples correctly, you have understood the basics of Scala's dynamic types!\n","link":"https://bigelectrons.com/post/scala/scala-dynamic-proxy/","section":"post","tags":null,"title":"Scala's Dynamic Proxy"},{"body":"Say that you have to use a Java library in your Scala application and this Java library has a couple of API's that require you to pass a Java collection type. Let's see how we can cope with it by looking at the following examples:\nSuppose I have a Java ArrayList that contains some Integers that I want to map these List of Integers by incrementing 1 to each of the elements in the ArrayList as below:\n1scala\u0026gt; val javaList = new java.util.ArrayList[Integer]() 2javaList: java.util.ArrayList[Integer] = [] 3 4scala\u0026gt; javaList.add(1) 5res0: Boolean = true 6 7scala\u0026gt; javaList.add(2) 8res1: Boolean = true 9 10scala\u0026gt; javaList map { i =\u0026gt; i + 1 } 11error: value map is not a member of java.util.ArrayList[Integer] 12 javaList map { i =\u0026gt; i + 1 } As you can see that the message from the Scala compiler is very clear that there is no map function available to the ArrayList that we need someway to tell the compiler to treat that as a Scala collection. Java Conversions to the rescue:\n1import scala.collection.JavaConversions._ 2 3scala\u0026gt; javaList map { i =\u0026gt; i + 1 } 4res3: scala.collection.mutable.Buffer[Int] = ArrayBuffer(2, 3) What happened above is that we imported the JavaConversions package available in the Scala library to tell the compiler that it should implicitly convert the java.util.ArrayList to a closest possible collection type in Scala which in our case happened to be an ArrayBuffer. If you take a look at the Scala's API documentation on JavaConversions there is an implicit method called asScalaBuffer which was used in our example implicitly to convert the java.util.ArrayList to a Scala collection type.\nThe JavaConversions package contains a couple of implicit methods to convert between a Java collection to a Scala collection and vice-versa. There is yet another package in the scala.collections called JavaConverters. What is the difference? Both the JavaConversions and JavaConverters do more or less the same thing, i.e., convert to and fro between a Java / Scala collection. You might be wondering over which one you should prefer. Let's find that out with the help of some more examples:\n1scala\u0026gt; val scalaMap = Map(\u0026#34;A\u0026#34; -\u0026gt; 1) 2scalaMap: scala.collection.immutable.Map[String,Int] = Map(A -\u0026gt; 1) 3 4scala\u0026gt; scalaMap.contains(1) 5error: type mismatch; 6found : Int(1) 7required: String 8scalaMap.contains(1) 9^ What we did just now is instantiated an immutable Map from the Scala library that maps a set of String to Int. Remember, the syntax for Scala Map looks like below:\n1trait Map[A, +B] extends Iterable[(A, B)] with GenMap[A, B] 2 with MapLike[A, B, Map[A, B]] The contains method looks like below:\n1def contains(key: A): Boolean What we did with our scalaMap variable above is that we created a Map that took a String as key and Int as value, so when we called the contains method and passed it an Int, it failed. The contains method implementation in Scala internally calls the get method to check if the key is contained in the collection. Now let's see what happens if we import the JavaConversions:\n1scala\u0026gt; import scala.collection.JavaConversions._ 2import scala.collection.JavaConversions._ 3 4scala\u0026gt; scalaMap.contains(1) 5res1: Boolean = false It kind of worked and returned false. Practically speaking the variable scalaMap does not contain 1, so returning false would look like the right thing to do. What was compromised here is the type safety that we had before importing the JavaConversions. Soon after importing the JavaConversions package, the Scala compiler gives an implicit call to the MapWrapper class wherein the MapWrapper class implements the containsKey method from the Java library. The implementation looks like this in the Scala library's MapWrapper class\n1override def containsKey(key: AnyRef): Boolean = try { 2// Note: Subclass of collection.Map with specific key type may redirect generic 3// contains to specific contains, which will throw a ClassCastException if the 4// wrong type is passed. This is why we need a type cast to A inside a try/catch. 5underlying.contains(key.asInstanceOf[A]) 6} catch { 7 case ex: ClassCastException =\u0026gt; false 8} As it can be seen, the containsKey takes an Object / AnyRef rather than the specific type which in our case is a String. With JavaConversions, we went down from a stricter type to a more non stricter type. Let's see now how we could retain the stricter type by being a bit more explicit, using the JavaConverters!\n1scala\u0026gt; val scalaMap = Map(\u0026#34;A\u0026#34; -\u0026gt; 1) 2scalaMap: scala.collection.immutable.Map[String,Int] = Map(A -\u0026gt; 1) 3 4scala\u0026gt; import scala.collection.JavaConverters._ 5import scala.collection.JavaConverters._ 6 7scala\u0026gt; scalaMap.contains(1) 8error: type mismatch; 9found : Int(1) 10required: String 11scalaMap.contains(1) 12^ Did you see the difference? There is now no implicit conversion between the Java and Scala collection types. If you want the scalaMap variable to be a java.util.Map, you now have to be explicit as below:\n1scala\u0026gt; scalaMap.asJava 2res0: java.util.Map[String,Int] = {A=1} So from my point of view, it is better to be explicit which would imply to use the JavaConverters instead of JavaConversions.\n","link":"https://bigelectrons.com/post/scala/scala-java-conversion/","section":"post","tags":null,"title":"Converting collections to and fro in Scala / Java"},{"body":"Having worked with C# in the last couple of months, writing extension methods was one of the cool features that I appreciated a lot!\nExtension methods are just a way to add new methods to the existing set of types. For example, the double type has several methods as part of the Scala API. If you now want to add a method that finds the reciprocal of a double, the first thing that comes naturally would be to write a helper utility function that does this for us. With extension methods, you can make that helper utility function look like as though it is part of the API itself. So let's see how to write the reciprocal extension method on a double!\n1implicit class DoubleExtensions(val d: double) extends AnyVal { 2 def reciprocal = 1 / d 3} A few points to note on the code snippet\nThe implicit keyword makes the primary constructor of the class available for implicit conversions The implicit classes may be defined inside a trait, class or an object - good practice Only one non-implicit argument is allowed in the primary constructor (the val d: double in the case above) There may not be any method, member or object in scope with the same name as the implicit class I find this pretty neat and very simple rules surrounding implicit classes!\n","link":"https://bigelectrons.com/post/scala/scala-implicit-class/","section":"post","tags":null,"title":"Implicit Classes Scala 2.10 Extension Methods"},{"body":"One of the astonishing features that Scala has is the pattern matching mechanism. Think of it like a Java Switch statement on Steroids. Pattern matching is such a powerful concept in Scala. Once you start using them, you'll find it inevitable to not using them. Let's look at some examples\nYou want to pattern match a List based on the number of elements in the List :\n1def patternMatch = { 2 val matchResult = List(1,2,3,4,5) match { 3 case Nil =\u0026gt; \u0026#34;Matches an empty List\u0026#34; 4 case head :: Nil =\u0026gt; \u0026#34;Matches a List that has exactly one element\u0026#34; 5 case head :: tail =\u0026gt; \u0026#34;Matches a List that hast at-least one element\u0026#34; 6 } 7} What is happening in the above code snippet is that, the myList is run through the case blocks. It is obvious that myList has more than one element in it and hence the last case statement is matched where the following statement is returned as the matchResult.\nIf you are wondering what the case syntax is all about, here is a quick description of what it is:\n1case \u0026lt;i\u0026gt;pattern\u0026lt;/i\u0026gt; =\u0026gt; \u0026lt;i\u0026gt;expression\u0026lt;/i\u0026gt; The case is a Scala keyword, the pattern is what you specify for the supplied value to be checked against and the expression is what you want to be returned if the pattern matches. Simple or?\n","link":"https://bigelectrons.com/post/scala/scala-pattern-match-collection/","section":"post","tags":null,"title":"Pattern Matching Scala Collections"},{"body":"No, it is not the Map implementation that we are going to talk about. Let me ask you a very simple question. I assume that you have written functions or methods or routines or whatever you call it. What do you think that it actually does? ...., 3...., 2...., 1.... TimeUp. Let me answer that for you.\nA function or a method maps the thingy on the left to the thingy on the right. The thingy on the left is your function argument, the thingy on the right is the value that the function computed for you using the argument you supplied it.\n1myFunction(leftThingy) = { 2... 3val rightThingy = doSomething() 4... 5rightThingy 6} So effectively speaking, a function maps one thing to the other. That's all to it. Now what relevance has this got to do with Scala? In Scala, map is a higher order function that applies a function to each of the parameter that you pass to it and returns the results. A piece of code snippet is a lot better I suppose.\nLet's say you have a List of Int's. You now want to calculate the square of each element in the List and return the result as a List. Let's start with defining a square function that takes an Int and returns the square of that Int. Effectively, you are mapping an Int value to the square of it which again is an Int value. Your square function maps an Int to an Int (Int =\u0026gt; Int in Scala syntax)\n1def square(x: Int): Int = x * x 2... 3... 4val myList = List(1,2,3,4) 5val mySquareList = myList.map(square) 6... 7... 8println(mySquareList) //would print List(1, 4, 9, 16) 9... 10... What happened was, you mapped a set of inputs to a set of output by applying a function to each of the input values. Did you get that? If not, I'm stupid you are brilliant.\n","link":"https://bigelectrons.com/post/scala/scala-map/","section":"post","tags":null,"title":"Map'em up"},{"body":"An ExecutionContext or otherwise called a thread pool is vital when running asynchronous code. In Scala, there is a global ExecutionContext that you can supply to your asynchronous code blocks. An ExecutionContext is just an Executor of a Task. It executes a Task in such a way that it is free to execute the Task in a new Thread of its choice.\nThe Scala's global ExecutionContext uses a ForkJoinPool. The ForkJoinPool is an ExecutorService that executes a Task. Understanding and demystifying the ForkJoinPool is out of scope for this blog, but I will just highlight what it does to your asynchronous code block:\nIt makes your execution to run in parallel Unlike other ExecutorService, the ForkJoinPool keeps an explicit association between tasks So, the bare minimum that you have to know about the ForkJoinPool is that it is able to run small computations with little thread overhead!\nNow when you wrap a piece of code in a Future block in Scala, there should be somebody, some thread to run that bit of code that is contained in your Future block. So from where does this thread come from? This is exactly where the global ExecutionContext from Scala pitches in.\nThere is yet another concept that is worth knowing about when using Future's and ExecutionContext's in Scala. Say you have a very slow computation that might cost you a lot of Thread time, how could you indicate this message about the upcoming slow computation to the run time so that this slow computation is given a special consideration and is made to run in such a way that you do not run out of threads when you have many of those slow computations waiting to happen. Enter the BlockingContext! Whack... yet another XXXContext!!!! Marking a piece of code in Scala in a BlockingContext is as simple as:\n1scala.concurrent.blocking { ... } When you wrap your code in the blocking { ... } block, Scala places that block in a BlockingContext. What happens after that is better explained in the form of a dialogue that goes as:\nBlockingContext - Hey, Current Thread, do you have to do anything before I'm about to start this slow thingy? Current Thread - Oh... yeah, let me inform the ForkJoinPool about this ForkJoinPool - I will give a special new Thread to handle this slow thingy, I will also defer this BlockingContext to the managedBlock in my pool\nYou see, this is essentially one of the best practices that should be followed when dealing with slow running computations. Just remember to wrap them inside the blocking context!\nSo effectively Scala's Future's and its global ExecutionContext empowers you to write parallely executing code, keeps your CPU machinery in steam without going Thread-frenzy.\n","link":"https://bigelectrons.com/post/scala/scala-execution-context/","section":"post","tags":null,"title":"What the heck are Scala's ExecutionContext's?"},{"body":"Before taking a look at what partial functions in Scala is, let's look at some examples that map an assorted List of elements:\n1 val assorted = List(1,2,\u0026#34;joe\u0026#34;,\u0026#34;san\u0026#34;) 2 assorted map { case i: Int =\u0026gt; i + 1 } The code above when tried results in a match error. The reason being that we have an anonymous function inside the map block that checks against each element and since the match is not exhaustive, it explodes with an error. Let's try the same example with the collect method available in the Scala collection library\n1 val assorted = List(1,2,\u0026#34;joe\u0026#34;,\u0026#34;san\u0026#34;) 2 assorted collect { case i: Int =\u0026gt; i + 1 } // gives List(2,3) Why did map fail while collect did what we wanted? If you look at the method signatures from the Scala API, it is pretty obvious that collect takes in something called as a partial function, which becomes the topic of this article. Here is my explanation and understanding of what a partial function is:\nA partial function is any function that is defined for a subset of its input domains. Let's take a small example to understand this:\n1 def add(i: Int) = i + i The input domain to this simple add function is the set of all integers. The add function holds good for the set of all integers, i.e., across the whole integer domain. You pass it any input int value, you will get a result which is also an int in this case. Let's take another case, but remember the fact that the function should work for any input in that particular domain and produce us a result.\n1 def divide(i: Int) = 1 / i What happens if I call divide(0)? We should have got a result, but that did not happen. Instead we failed with an error or an exception. So how do I define this divide function in plain sentence? The divide function divides the input for a set of all partial input domains! Does that make sense? The divide function works only for a partial set of integer domain. Let's now fix the divide function to make it cope up with the failure (divide(0))... no no not with try catch!, but with Scala's built-in support for defining partial functions!\nOur refactored divide function as a partial function can be written as below:\n1 val divide = new PartialFunction[Int, Int] { 2 def apply(x: Int) = 1 / x 3 def isDefinedAt(x: Int) = x != 0 4 } We have now made the divide function to be a partial function. We can now check if the divide function operates for any of the input domain and decide if we call the function or not. The following example illustrate this:\n1 divide.isDefinedAt(0) // tells us that it is not supported by returning false! Let's now rewrite the same example but this time with a case block! This is just a Scala shorthand way to define a partial function.\n1 val divide: PartialFunction[Int, Int] = { 2 case i: Int if i != 0 =\u0026gt; 1 / i 3 } Where did our apply and isDefinedAt go with our re-written partial function example? If you are just curios, try compiling your code and have a look at how this is expanded upon compilation. I'll leave that to you! If you are a bit lazy to look into the compiled code, have a look at this post:\nThe Scala's partial function trait has the following signature:\n1 trait PartialFunction[-A, +B] extends (A) ⇒ B The definition as per Scala docs, a partial function is any function where the domain does not necessarily include all values of type A. Anybody who implements the partial function trait has to provide implementations for apply and isDefinedAt abstract functions which takes the type A and defines if that instance of type A belongs to the domain covered by the partial function. So that wraps our discussion on partial functions in Scala!\n","link":"https://bigelectrons.com/post/scala/scala-partial-functions/","section":"post","tags":null,"title":"Partial Functions in Scala"},{"body":"If you are like me that is fed-up with writing or generating those getters / setters in your domain objects or Data Transfer Objects or Value Objects, deferring the process of writing at compile time is definitely a boon. Scala's case classes does exactly that. Some noteworthy things to know about Scala case classes.\nThey are defined with the keyword case and are always immutable in nature because the parameters are val's implicitly! 1case class Person(name: String, age: Int) Now you could of course make the parameter as var, but be warned that your equals and hashCode behavior that you implicitly get is not bound to do what you wanted it to do\nThe parameters that are passed to the case classes are treated as final and are instantiated in the class constructor\nSimple to instantiate! You could also define some defaults to the parameters if you do not want to supply them each and every time\n1case class Person(name: String = \u0026#34;MyName\u0026#34;, age: Int) 2val person = Person(29) // Would instantiate a person with name = MyName and age = 29 Gives you implicitly defined equals and hashCode implementation\nYou can decompose them using Scala's pattern matching feature, the reason you can pattern match a case class is because you also get the unapply method implemented for all your case classes\nThe getter methods are generated by default for you and if you have any var's as parameters, a setter method is also generated for that parameter that is declared as a var. But please don't try to mutate in a case class!\nWith all that said, let's now do a simple pattern matching snippet using a case classes:\n1abstract class Animal(age: Int) 2case class Dog(name: String, age: Int) extends Animal(age) 3case class Elephant(name: String, age: Int, sex: String) extends Animal(age) We have now defined two case classes each with its own set of properties. Here is how pattern matching could then be applied:\n1def patternMatchAnimal(animal: Animal) = animal match { 2 case d: Dog =\u0026gt; println(\u0026#34;that was just Any Dog\u0026#34;) 3 case d: Dog(\u0026#34;MyDog\u0026#34;, _) =\u0026gt; println(\u0026#34;that was indeed My Dog\u0026#34;) 4 case e: Elephant(_, 100, _) =\u0026gt; println(\u0026#34;Wow! that elephant lived 100 years\u0026#34;) 5 case _ =\u0026gt; println(\u0026#34;Were you a human! well not really\u0026#34;) 6} 7 8patternMatchAnimal(Dog(\u0026#34;AnyDog\u0026#34;, 2)) // prints -\u0026gt; that was just Any Dog 9patternMatchAnimal(Dog(\u0026#34;MyDog\u0026#34;, 2)) // prints -\u0026gt; that was indeed My Dog 10patternMatchAnimal(Elephant(\u0026#34;Ele\u0026#34;, 100, \u0026#34;M\u0026#34;)) // prints -\u0026gt; Wow! that elephant lived 100 years 11patternMatchAnimal(Elephant(\u0026#34;Ele\u0026#34;, 101, \u0026#34;M\u0026#34;)) // prints -\u0026gt; Were you a human! well not really Having said that, there is much more to case classes. Have a look at any Scala text to read more on Scala case classes!\n","link":"https://bigelectrons.com/post/scala/scala-case-class/","section":"post","tags":null,"title":"Making a Case for Scala's case classes"},{"body":"This blog article takes a deeper look into Scala's for comprehensions. The for comprehension / expression in Scala has the following syntax:\n1 for ( seq ) yield expr In deed a very simple syntax underneath which lies a very strong fundamental concept. The seq in the syntax above could be a sequence of generators, definitions, and filters each separated with a semicolon. Let us assume that we have to identify all the Persons from a List that matches a certain criteria. Of course this is Scala and there are zillions of way to do this, but the focus of this article is to discuss about for expressions. Let's now see how we can accomplish this in just 2 lines of code.\n1 val persons = List(person1, person2, person3) 2 for ( person \u0026lt;- persons; age = person.age; if(age \u0026gt; 18) ) yield person 3 // for (generator; definition; filter) yield value In fact that was indeed a one liner. What we did was that we filtered for all persons that are 18 years or more old. Let's focus on each one of those sequences. Let's begin with the generator.\nThe generator syntax pat \u0026lt;- expr, where expr typically returns a type that can be iterated over. In simple terms, a generator is anything or rather any type that has a method map, flatMap and a filter. Let us verify this ourselves with the help of the following code:\n1 val myInt = 10 2 for ( i \u0026lt;- myInt; if(i \u0026gt; 10) ) yield i 3 error: value filter is not a member of Int 4 for ( i \u0026lt;- myInt; if(i \u0026gt; 10)) yield i 5 ^ The code above fails for obvious reasons that type Int does not contain a filter method defined and hence does not qualify to be as a generator. Let's now modify it slightly and see what happens:\n1 val myIntList = List(1,2,3,4,5) 2 for ( i \u0026lt;- myIntList; if(i \u0026gt; 4) ) yield i // returns List(5) The myIntList now qualifies as a generator as it satisfies the 3 conditions that we laid out earlier for a generator. Before going further, let me leave you with a slightly more complicated example. I will leave it to you so that you can figure that out by yourselves. If you have done nested for loops in Java, the answer should be pretty obvious.\n1 val list1 = List(0,1,2,3,4) 2 val list2 = List(5,6,7,8,9) 3 for ( i \u0026lt;- list1; x \u0026lt;- list2; if(i \u0026gt;= 4) ) yield i // ??? What happens? Revisiting our Person list example:\n1 val persons = List(person1, person2, person3) 2 for { 3 person \u0026lt;- persons 4 age = person.age 5 if (age \u0026gt; 18) 6 } yield person The generator iterates through the person list and for each person in the list, the age of that person is extracted into a local val age, the age is checked if it is greater than 18, if yes that person is added to the result of the for expression.\n","link":"https://bigelectrons.com/post/scala/scala-for-expression/","section":"post","tags":null,"title":"Scala's for expression"},{"body":"Scala seems to be like a gold mine, the more you dig into, the more you get. While you deal with tuples, when you want to access the elements of a tuple, you have to use the default numbering given to you by the Scala compiler. Say for example., you have a tuple as defined below:\n1val myTuple = (31, \u0026#34;Joesan\u0026#34;, \u0026#34;M\u0026#34;) 2println(myTuple._1) // To access the first element What you could alternatively do is to decompose the tuple into its constituent elements as below:\n1val myTuple = (31, \u0026#34;Joesan\u0026#34;, \u0026#34;M\u0026#34;) 2val (age, name, sex) = myTuple 3println(age) // To access the age set in myTuple 4println(name) // Did you get the idea... I find this way of decomposing the tuple and accessing the elements from it using some meaningful names much more elegant than fiddling around with numbers which unfortunately does not say much on what you are reading.\n","link":"https://bigelectrons.com/post/scala/scala-accessing-tuples/","section":"post","tags":null,"title":"Accessing Tuples in Scala"},{"body":"You probably know the idea behind using case classes and pattern matching them. Sometimes you might want the help of the compiler to tell you if you have pattern matched correctly by covering all possible cases. You can now get this sort of compiler support by creating a superclass for the case classes and marking them as sealed.\n1sealed trait Person 2case class Student(name: String) extends Person 3case class Teacher(name: String) extends Person 4case class Author(name: String) extends Person 5case class Bertender(name: String) extends Person 6... 7... 8def printAPerson(person: Person) = person match { 9 case p @@ Student(_) =\u0026gt; println(p) 10 case p @@ Teacher(_) =\u0026gt; println(p) 11} With the code above, the compiler would now tell us that the match is not exhaustive and would show us the following:\n1warning: match may not be exhaustive. 2It would fail on the following inputs: Author(_), Bertender(_) 3def printAPerson(person: Person) = person match {...} Sealed traits or abstract classes is a guarantee for the compiler to know that all the classes that implement the sealed trait or sealed abstract class should be declared in the same file. This way, the compiler knows for sure that all the possible subclasses is in the same file and can therefore provide us with the warning message on our pattern match logic.\nNow it's time for you to go figure out the meaning of the following code snippet:\n1def printAPerson(person: Person) = (person: @@unchecked) match { 2 case p @@ Student(_) =\u0026gt; println(p) 3 case p @@ Teacher(_) =\u0026gt; println(p) 4} ","link":"https://bigelectrons.com/post/scala/scala-sealed-traits/","section":"post","tags":null,"title":"Sealed Traits in Scala"},{"body":"In mathematics, linear interpolation is understood as a method of curve fitting using linear polynomials. A polynomial is just an expression that consists of variables along with their co-efficients. An example of a polynomial would be:\n1x+2y\u0026amp;sup2 In the above equation, the variable x has 1 as its coefficient, the variable y has 2 as its coefficient. Let us assume that we have a set of linear data points pointing to the x and y-axis:\n1x = {2,5,7,10,11,15,17,19,21,25} 2y = {3,6,8,10,14,16,19,21,23,26} Given a value x, we can find the value y. For example., for x = 2, the corresponding y is 3, for x = 11, the corresponding y is 14. Now we are interested in finding the value of x and y within our set of data points.\nWhat is the value of y for x = 13 or what is the value of x for y = 13. Since the data points for both the x and y co-ordinates are linear, we can use the linear interpolation technique to find out the values for the desired data points. Geometrically, the linear interpolant is just a straight line between two known data points. Let us assume that our two data points are represented as (x0,y0) and (x1,y1). We are interested in finding the values of x and y that lies between these data points. Since we are in the linear boundary, geometrically speaking the ratio of the difference between x and y co-ordinates between two data points are equal.\nLet us try to ascertain this with some numbers. Let our data points (x0,y0) be (2,3) and (x1,y1) be (5,6). The ratio y1/x1 should be equal to the ration y0/x0 and in our case, 6/5 == 3/2. Extending this idea, we can now say that the ratio of the difference between two data points should be geometrically equal between the two co-ordinates. If we are interested in a data point (x,y) which lies in-between (x0,y0) and (x1,y1), our formula now becomes:\n1y - y0 / x - x0 = y1 - y0 / x1 - x0 which when solving for y, we get the following equation: 2y = y0 + (y1 - y0) * (x - x0) / (x1 - x0) We now have the formula for linear interpolation, let's now implement that in Scala. Let us represent our data points as a Scala List:\n1 val x = List(2,5,7,10,11,15,17,19,21,25) 2 val y = List(3,6,8,10,14,16,19,21,23,26) 3 val zippedDataPoints = x zip y We define a function that takes these two zipped List of data points and an additional parameter for x. The function uses the formula to find the value of y for a given x. Here is the function:\n1def interpolate(givenX: Int, dataPoints: List[(Int, Int)]) = { 2 // edge cases 3 val (headX, headY) = dataPoints.head 4 val (lastX, lastY) = dataPoints.last 5 givenX match { 6 case a if givenX \u0026gt;= lastX =\u0026gt; lastY 7 case b if givenX \u0026lt;= headX =\u0026gt; headY 8 case c =\u0026gt; { 9 val (lower,upper) = dataPoints.span { case (x,y) =\u0026gt; x \u0026lt; givenX } 10 val (x0,y0) = lower.last 11 val (x1,y1) = upper.head 12 // finally our formula! 13 y0 + (y1 - y0) * (givenX - x0) / (x1 - x0) 14 } 15 } 16} Let's now write some tests to test our interpolation (using ScalaTest):\n1 interpolate(2, zippedDataPoints) == 3 2 interpolate(4, zippedDataPoints) == 5 3 interpolate(27, zippedDataPoints) == 26 So there it is! we have our linear interpolation implemented using Scala!\n","link":"https://bigelectrons.com/post/math/liner-interpolation/","section":"post","tags":null,"title":"Liner Interpolation"},{"body":"If you agree with me that writing less code equals lesser bugs, you are going to appreciate how Scala simplifies Strategy and Builder design patterns by making you write less code!\nLet's look at them one by one. Let us see how the Builder design pattern is implemented in Java.\nBuilder Pattern Builder design pattern helps in constructing a complex object in a series of steps. Let me take a classical example of preparing a Pizza, especially a vegetable Pizza! Our Pizza modelled as a POJO would look like below! Assume that we have the needed getters and setters to instantiate a Pizza.\n1public class VegetarianPizza extends Pizza { 2 private String dough; 3 private Enum sauceType; 4 private Enum cheeseType; 5 private String toppings; 6 private Enum sizeType; 7 private Boolean isTomatoAdded; 8 private Boolean isGarlicAdded; 9 private Boolean isOnionAdded; 10 private Boolean isSpinachAdded; 11} Now as I see the instance fields, I can see 9*9 ways of instantiating a VegetarianPizza. I do not definitely want to write those many constructors! Builder pattern to the rescue!\nHere is how I could add a VegetarianPizza builder to make a VegetarianPizza instantiation much organised:\n1public class VegetarianPizza extends Pizza { 2 private String dough; 3 private Enum sauceType; 4 private Enum cheeseType; 5 private String toppings; 6 private Enum sizeType; 7 private Boolean isTomatoAdded; 8 private Boolean isGarlicAdded; 9 private Boolean isOnionAdded; 10 private Boolean isSpinachAdded; 11 12 //Private constructor to enforce instantiation via Builder 13 private VegetarianPizza(...) { ... } 14 public static class VegetarianPizzaBuilder { 15 private String dough; 16 private Enum sauceType; 17 private Enum cheeseType; 18 private String toppings; 19 private Enum sizeType; 20 private Boolean isTomatoAdded; 21 private Boolean isGarlicAdded; 22 private Boolean isOnionAdded; 23 private Boolean isSpinachAdded; 24 25 public VegetarianPizzaBuilder withDough(String dough) { 26 this.dough = dough; return this; 27 } 28 29 public VegetarianPizzaBuilder withSauceType(Enum sauceType) { 30 this.sauceType = sauceType; return this; 31 } 32 33 public VegetarianPizzaBuilder withcheeseType(Enum cheeseType) { 34 this.cheeseType = cheeseType; return this; 35 } 36 37 public VegetarianPizzaBuilder withTopping(String toppings) { 38 this.toppings = toppings; return this; 39 } 40 41 public VegetarianPizzaBuilder withSize(Enum sizeType) { 42 this.sizeType = sizeType; return this; 43 } 44 45 public VegetarianPizzaBuilder withTomato(Boolean isTomatoAdded) { 46 this.isTomatoAdded = isTomatoAdded; return this; 47 } 48 49 public VegetarianPizzaBuilder withGarlic(Boolean isGarlicAdded) { 50 this.isGarlicAdded = isGarlicAdded; return this; 51 } 52 53 public VegetarianPizzaBuilder withOnion(Boolean isOnionAdded) { 54 this.isOnionAdded = isOnionAdded; return this; 55 } 56 57 public VegetarianPizzaBuilder withSpinach(Boolean isSpinachAdded) { 58 this.isSpinachAdded = isSpinachAdded; return this; 59 } 60 61 public Pizza buildVegetarianPizza() { 62 return new VegetarianPizza(this); 63 } 64 } 65} That was roughly 60+ lines of boiler plate code. Ain't that shitty!\nEnter Scala, using case classes with default parameters, our VegetarianPizza in Scala would look like\n1case class VegetarianPizza(dough: String = \u0026#34;WhiteDough\u0026#34;, 2 sauceType: SauceType = SauceType.withName(\u0026#34;Ketchup\u0026#34;), 3 cheeseType: CheeseType = CheeseType.withName(\u0026#34;Irish\u0026#34;), 4 toppings: String = \u0026#34;Anything\u0026#34;, 5 sizeType: SizeType = SizeType.withName(\u0026#34;Large\u0026#34;), 6 isTomato: Boolean = true, 7 isGarlic: Boolean = true, 8 isOnion: Boolean = true, 9 isSpinach: Boolean = true 10) That's all it. Our builder is reduced to less than 10 lines! You can instantiate it like this!\n1val defaultVegPizza = VegetarianPizza() 2val withoutOnions = VegetarianPizza(isOnion = false) 3val withoutSpinachAndTomato = VegetarianPizza(isTomato = false, isSpinach = false) Strategy Pattern Let's now dive into the Strategy pattern and see what Scala has to offer in terms of brevity The definition of the strategy pattern goes like this \u0026quot;... enable an algorithm's behavior to be determined at runtime. Let me take the classical example of doing something with two Integers. What could be done with them? Add, Subtract, Multiply or Divide! Let's dive into some code. Let's first do it the ugly way using Java:\n1interface Strategy { 2 int execute(int a, int b); 3} Our concrete implementations for Add, Subtract, Multiply and Divide\n1class Add implements Strategy { 2 public int execute(int a, int b) { 3 return a + b; 4 } 5} 6 7class Subtract implements Strategy { 8 public int execute(int a, int b) { 9 return a - b; 10 } 11} 12 13class Multiply implements Strategy { 14 public int execute(int a, int b) { 15 return a * b; 16 } 17} 18 19class Divide implements Strategy { 20 public int execute(int a, int b) { 21 return a / b; 22 } 23} We now wrap our strategy in a context object, so that the clients can call our strategy depending on the context of what they want to do with the strategy.\n1class Context { 2 private Strategy strategy; 3 4 public Context(Strategy strategy) { 5 this.strategy = strategy; 6 } 7 8 public int executeStrategy(int a, int b) { 9 return this.strategy.execute(a, b); 10 } 11} All our client or the caller of our strategy needs to do is to get a context, pass in the strategy to the context and execute the strategy within that context.\nHere it is in code!\n1 Context ctx = new Context(new Add()); 2 ctx.executeStrategy(1, 2); All good so far. If you watch closely, it is all about invoking the algorithm that does the actual job. What is an algorithm? In our example above it is just our Add, Subtract, Multiply and Divide functions. What? Did I just say functions .... functional programming.... Scala!\nLet's now see how Scala addresses this with a functional approach\n1object IWillNotWriteBoilerPlateStrategyPatternAnymore extends App { 2 def add(a: Int, b: Int) = a + b 3 def sub(a: Int, b: Int) = a - b 4 def mul(a: Int, b: Int) = a * b 5 def div(a: Int, b: Int) = a / b 6 7 def execute(fn: (Int, Int) =\u0026amp;gt; Int, a: Int, b: Int) = fn(a, b) 8} That is all it! This is what we call simplicity! Here is how to call it!\n1execute(add, 1, 2) 2execute(mul, 1, 2) So I hope that I was able to convince you with some historical design patterns and how you could simplify them with a functional programming approach. Now, since Java 8 has functional paradigm baked into the language, I still find Scala's functional approach is lot simpler and subtler! The choice is your's!\n","link":"https://bigelectrons.com/post/scala/scala-builder-strategy-pattern/","section":"post","tags":null,"title":"Builder, Strategy Patterns Still relevant?"},{"body":"","link":"https://bigelectrons.com/categories/engineering/","section":"categories","tags":null,"title":"engineering"},{"body":"This page exists for one single fact that upon a glance on the topic, you get your aha moment!\nScala Co-Variant \u0026amp; Contra-Variant Spark SQL Joins ","link":"https://bigelectrons.com/post/facts/facts/","section":"post","tags":null,"title":"Facts \u0026 Knowledge Nuggets"},{"body":"","link":"https://bigelectrons.com/tags/index/","section":"tags","tags":null,"title":"index"},{"body":"","link":"https://bigelectrons.com/categories/ml/","section":"categories","tags":null,"title":"ml"},{"body":"","link":"https://bigelectrons.com/categories/navo/","section":"categories","tags":null,"title":"navo"},{"body":"Updated on: 19.12.2021\nThis website does not collect any personally identifiable information that can be used to personally identify you. There are no cookies installed that could be used to individually track you.\nTo learn more about cookies, please visit this website here.\nLinks to third party websites This website may contain links to other sites that are not in any way owned or managed by me or this website. Please be aware when you click on any such links, I'm not responsible for the privacy practices and/or policies of any website other than this website. Consequently, I strongly suggest that you review the privacy policy of every website you visit, whether they are linked to by this website.\n","link":"https://bigelectrons.com/post/privacy/privacy-policy/","section":"post","tags":null,"title":"Privacy Policy"},{"body":"","link":"https://bigelectrons.com/categories/programming/","section":"categories","tags":null,"title":"programming"},{"body":"","link":"https://bigelectrons.com/categories/projects/","section":"categories","tags":null,"title":"projects"}]