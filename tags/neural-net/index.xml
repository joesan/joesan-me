<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>neural-net on bigelectrons</title><link>https://bigelectrons.com/tags/neural-net/</link><description>Recent content in neural-net on bigelectrons</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2018 - 2025 @ https://github.com/Joesan</copyright><lastBuildDate>Tue, 10 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://bigelectrons.com/tags/neural-net/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Feed Forward Neural Network Architectures</title><link>https://bigelectrons.com/post/math/feed-forward-neural-net/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://bigelectrons.com/post/math/feed-forward-neural-net/</guid><description>
I have been reading through the architectures of Neural Networks and wanted to grasp the idea behind calculating the weights in a Neural Network and as you can see in the image below is a simple 2 node 2 layer Neural Network.
As you can see that for simplicity’s sake, I have just used a 2 node 2 layer network, but the same holds true for any sized Neural Network. It's just that the size of the Matrix increases proportionally to the number of inputs!</description></item></channel></rss>